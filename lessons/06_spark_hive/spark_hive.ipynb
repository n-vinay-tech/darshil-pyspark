{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sparkSQL\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ Spark SQL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 1: What is Spark SQL?\n",
    "\n",
    "- **Spark SQL** lets you run SQL queries on top of Spark DataFrames and tables.\n",
    "- You can use:\n",
    "    - **ANSI-SQL** (standard SQL syntax).\n",
    "    - **HiveQL** (Hive-compatible syntax).\n",
    "- Queries run on Spark's distributed engine and can use **system functions, UDFs, and optimizations**.\n",
    "\n",
    "ðŸ‘‰ Before Spark, Hive was the de facto SQL layer for big data. Spark SQL replaced Hive in many systems because:\n",
    "\n",
    "- It's faster,\n",
    "- It integrates directly with DataFrames,\n",
    "- It can connect to Hive metastores for compatibility.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 2: Running SQL Queries in Spark\n",
    "\n",
    "**Simple SQL command:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(1 + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Register DataFrame as a SQL view:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|   DEST_COUNTRY_NAME|total_count|\n",
      "+--------------------+-----------+\n",
      "|             Senegal|         40|\n",
      "|              Sweden|        118|\n",
      "|               Spain|        420|\n",
      "|    Saint Barthelemy|         39|\n",
      "|Saint Kitts and N...|        139|\n",
      "|         South Korea|       1048|\n",
      "|        Sint Maarten|        325|\n",
      "|        Saudi Arabia|         83|\n",
      "|         Switzerland|        294|\n",
      "|         Saint Lucia|        123|\n",
      "|               Samoa|         25|\n",
      "|        South Africa|         36|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = str(Path(get_project_path('data', 'darshil-data', 'flight-data', 'json', '2015-summary.json')))\n",
    "\n",
    "df = spark.read.json(json_path)\n",
    "df.createOrReplaceTempView(\"flights_view\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\n",
    "FROM flights_view\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "result.where(\"DEST_COUNTRY_NAME LIKE 'S%'\").where(\"total_count > 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ **Note:**\n",
    "\n",
    "- `createOrReplaceTempView` makes the DataFrame queryable via SQL.\n",
    "- You can then mix SQL and DataFrame API (`where`, `select`, etc.).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 3: Spark Tables\n",
    "\n",
    "- **Table = Persistent data structure** managed by Spark.\n",
    "- **DataFrame = Temporary, in code**.\n",
    "- Key difference:\n",
    "    - DataFrame exists only in the program.\n",
    "    - Table lives inside a **database** (metadata tracked by Spark/Hive metastore).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 4: Managed vs Unmanaged Tables\n",
    "\n",
    "- **Managed Table:**\n",
    "    - Created with `saveAsTable`.\n",
    "    - Spark manages both **data and metadata**.\n",
    "    - Dropping table deletes the data.\n",
    "- **Unmanaged Table (External):**\n",
    "    - Created from files on disk (`CREATE EXTERNAL TABLE`).\n",
    "    - Spark manages **metadata only**.\n",
    "    - Dropping table does **not** delete files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 5: Creating Tables\n",
    "\n",
    "**Create table directly from files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI path: 'file:///c:/ds_analytics_projects/darshil_course/apache-pyspark/darshil-pyspark/data/darshil-data/flight-data/json/2015-summary.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to file URI\n",
    "json_path_uri = Path(json_path).as_uri()\n",
    "print(f\"URI path: '{json_path_uri}'\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS flights\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE flights (\n",
    "    DEST_COUNTRY_NAME STRING,\n",
    "    ORIGIN_COUNTRY_NAME STRING,\n",
    "    count LONG\n",
    ")\n",
    "USING JSON OPTIONS (path '{json_path_uri}')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   NULL|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   NULL|\n",
      "|              count|   bigint|   NULL|\n",
      "+-------------------+---------+-------+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     256|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if table exists and show its structure\n",
    "spark.sql(\"DESCRIBE flights\").show()\n",
    "\n",
    "# Show some sample data\n",
    "spark.sql(\"SELECT * FROM flights LIMIT 5\").show()\n",
    "\n",
    "# Count rows\n",
    "spark.sql(\"SELECT COUNT(*) FROM flights\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create table from a query:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS flights_from_select\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE flights_from_select\n",
    "USING parquet AS\n",
    "SELECT * FROM flights\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create unmanaged (external) table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI path: 'file:///c:/ds_analytics_projects/darshil_course/apache-pyspark/darshil-pyspark/data/darshil-data/flight-data-hive'\n",
      "Directory: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\\data\\darshil-data\\flight-data-hive\n",
      "Directory exists: True\n",
      "Files in directory:\n",
      "  - part-00000-tid-4721890993021653500-d8ef7f6b-e6e5-4451-af50-08281422f186-0-c000\n",
      "  - _committed_4721890993021653500\n",
      "  - _started_4721890993021653500\n",
      "  - _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "hive_path = str(Path(get_project_path('data', 'darshil-data', 'flight-data-hive')))\n",
    "# Convert to file URI\n",
    "hive_path_uri = Path(hive_path).as_uri()\n",
    "print(f\"URI path: '{hive_path_uri}'\")\n",
    "\n",
    "print(f\"Directory: {hive_path}\")\n",
    "print(f\"Directory exists: {os.path.exists(hive_path)}\")\n",
    "\n",
    "if os.path.exists(hive_path):\n",
    "    print(\"Files in directory:\")\n",
    "    for file in os.listdir(hive_path):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS hive_flights\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE hive_flights (\n",
    "    DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '{hive_path_uri+\"/\"}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 6: Inserting into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO flights_from_select\n",
    "SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
    "FROM flights\n",
    "LIMIT 20\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 7: Useful SQL Commands\n",
    "\n",
    "- **Describe a table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   NULL|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   NULL|\n",
      "|              count|   bigint|   NULL|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE flights_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Show partitions of a table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           partition|\n",
      "+--------------------+\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "|DEST_COUNTRY_NAME...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Create a partitioned table from existing table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE flights_from_select_partitioned\n",
    "USING PARQUET\n",
    "PARTITIONED BY (DEST_COUNTRY_NAME)\n",
    "AS SELECT * FROM flights_view\n",
    "\"\"\")\n",
    "\n",
    "# Now you can show partitions on the new table\n",
    "spark.sql(\"SHOW PARTITIONS flights_from_select_partitioned\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Refresh table metadata:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"REFRESH TABLE flights_from_select_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Repair partitions (Hive-style):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MSCK REPAIR TABLE flights_from_select_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Drop a table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE flights_from_select_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 8: Views\n",
    "\n",
    "- **View = Saved query (no data stored)**.\n",
    "- Types:\n",
    "    - **Session-specific view** (temp).\n",
    "    - **Database-specific view**.\n",
    "    - **Global view** (across sessions).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VIEW just_usa_view AS\n",
    "SELECT * FROM flights WHERE DEST_COUNTRY_NAME = 'United States'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 9: Databases\n",
    "\n",
    "- Databases organize tables.\n",
    "- Default database = `default`.\n",
    "- Create new database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE some_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ Then use with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE some_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 10: Why this matters?\n",
    "\n",
    "- Spark SQL unifies **SQL + DataFrame API**.\n",
    "- Supports Hive compatibility, external tables, and schema management.\n",
    "- Tables (managed/unmanaged) and views give flexibility in **data persistence**.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In simple words:**\n",
    "\n",
    "Spark SQL lets you run SQL on top of DataFrames, store them as tables (managed/unmanaged), and organize them into databases. Views = saved queries, tables = actual data, and Hive compatibility makes migration easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
