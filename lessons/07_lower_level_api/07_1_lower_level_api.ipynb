{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lowerLevelApi\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ Lower-Level APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 1: What Are Lower-Level APIs?\n",
    "Spark provides two \"layers\" of APIs:\n",
    "* **High-level APIs (Structured APIs):**\n",
    "   * DataFrame, Dataset, and Spark SQL.\n",
    "   * Provide declarative syntax, optimized by Catalyst optimizer.\n",
    "   * Easier, safer, and preferred in most cases.\n",
    "* **Low-level APIs:**\n",
    "   * **RDDs (Resilient Distributed Datasets):** Primitive distributed collections with transformations and actions.\n",
    "   * **Distributed shared variables:**\n",
    "      * **Broadcast variables** (read-only shared data across executors).\n",
    "      * **Accumulators** (write-only variables to aggregate results, e.g., counters).\n",
    "   * **SparkContext** (entry point to cluster-level functionality).\n",
    "\n",
    "ðŸ‘‰ Every **DataFrame operation** internally compiles down to **RDD transformations and actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 2: When Should You Use Low-Level APIs?\n",
    "Use RDDs, accumulators, or broadcast variables only when:\n",
    "1. **Functionality missing in Structured APIs**\n",
    "   * e.g., fine-grained control of data placement, custom partitioning, byte-level data manipulation.\n",
    "2. **Legacy code**\n",
    "   * Old Spark jobs written with RDDs still need maintenance.\n",
    "3. **Custom shared variable manipulation**\n",
    "   * e.g., debugging counters, global accumulators, broadcasting lookup tables.\n",
    "\n",
    "Otherwise, stick with **DataFrame/Dataset** APIs â€” they're optimized, less error-prone, and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 3: How to Use Low-Level APIs?\n",
    "### SparkContext\n",
    "* **SparkContext** is the main entry point for lower-level operations.\n",
    "* Available from `SparkSession` via `spark.sparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Parallelize Python collection into an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ SparkContext is responsible for:\n",
    "* Connecting to the cluster manager.\n",
    "* Requesting resources (executors).\n",
    "* Creating RDDs from local collections or external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 4: RDDs (Resilient Distributed Datasets)\n",
    "* **RDD = low-level distributed collection of objects.**\n",
    "* Immutable, partitioned across cluster.\n",
    "* Supports transformations (map, filter, flatMap) and actions (collect, count, reduce).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Transformation (lazy)\n",
    "rdd_squared = rdd.map(lambda x: x * x)\n",
    "\n",
    "# Action (executes)\n",
    "print(rdd_squared.collect())  # [1, 4, 9, 16, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 5: Distributed Shared Variables\n",
    "### Broadcast Variables\n",
    "* Allow sharing large read-only data across executors without sending it with every task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast value: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "broadcast_var = sc.broadcast([1, 2, 3])\n",
    "\n",
    "print(\"Broadcast value:\", broadcast_var.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "* Write-only variables used for aggregations like counters or sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulator value: 15\n"
     ]
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "\n",
    "def add_num(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "\n",
    "rdd.foreach(add_num)\n",
    "print(\"Accumulator value:\", accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 6: Why This Matters?\n",
    "* All Spark jobs eventually **compile down to RDD operations**.\n",
    "* Understanding low-level APIs helps in:\n",
    "   * Debugging query plans.\n",
    "   * Optimizing performance at the physical level.\n",
    "   * Maintaining old codebases.\n",
    "* But in modern Spark: **prefer Structured APIs** unless you really need the control.\n",
    "\n",
    "âœ… **In simple words:**\n",
    "High-level APIs (DataFrame/SQL) are the default and most efficient way to work in Spark. But under the hood, everything runs on **RDDs**. You only use RDDs, broadcast variables, or accumulators when you need fine-grained control, maintain legacy code, or share data across tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
