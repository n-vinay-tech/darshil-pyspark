{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ Resilient Distributed Dataset (RDD)\n",
    "\n",
    "**Subject:** Apache Spark\n",
    "\n",
    "**Topics:** #spark #bigdata #rdd\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 1: What is an RDD?\n",
    "\n",
    "- **RDD** = Resilient Distributed Dataset\n",
    "- The **original Spark API** (before DataFrames).\n",
    "- Represents an **immutable, partitioned collection** of objects that can be processed in parallel across the cluster.\n",
    "- Each record is just a **Python/Scala/Java object** (unlike DataFrames, which have schemas).\n",
    "\n",
    "ðŸ‘‰ All Spark workloads (DataFrames, SQL) compile down to **RDD operations** internally.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 2: Why (or When) to Use RDDs?\n",
    "\n",
    "Use RDDs **only when**:\n",
    "\n",
    "1. You need **fine-grained control** over data distribution (custom partitioning).\n",
    "2. You're maintaining **legacy codebases** from Spark 1.x.\n",
    "3. You need to **manipulate objects directly** (e.g., complex, unstructured formats).\n",
    "\n",
    "ðŸ‘‰ Otherwise: **always prefer DataFrames** â€” they're optimized (Catalyst optimizer, Tungsten execution engine, compressed storage).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 3: Types of RDDs\n",
    "\n",
    "1. **Generic RDD**: collection of objects.\n",
    "2. **Key-Value RDD**: (key, value) pairs, with extra functions like `reduceByKey`, `groupByKey`, `partitionBy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 4: Properties of an RDD\n",
    "\n",
    "Each RDD has:\n",
    "\n",
    "- **Partitions** (data split across cluster).\n",
    "- **Computation function** for each split.\n",
    "- **Dependencies** (on parent RDDs).\n",
    "- **Optional Partitioner** (for key-value RDDs).\n",
    "- **Preferred locations** (where partitions should be processed, e.g., data locality in HDFS).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 5: Creating RDDs\n",
    "\n",
    "### (a) From a DataFrame / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to RDD\n",
    "df = spark.range(10).toDF(\"id\")\n",
    "rdd = df.rdd.map(lambda row: row[0])\n",
    "print(rdd.collect())  # [0, 1, 2, ..., 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) From a Local Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Darshil']\n"
     ]
    }
   ],
   "source": [
    "words = spark.sparkContext.parallelize(\"My name is Darshil\".split(\" \"), 2)\n",
    "words.setName(\"myWordsRDD\")\n",
    "print(words.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) From a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apache Spark is great for big data processing.', 'RDDs are the low-level API in Spark.']\n",
      "[('file:/c:/ds_analytics_projects/darshil_course/apache-pyspark/darshil-pyspark/data/sample/sample_text_file_1.txt', 'Apache Spark is great for big data processing.\\r\\nRDDs are the low-level API in Spark.'), ('file:/c:/ds_analytics_projects/darshil_course/apache-pyspark/darshil-pyspark/data/sample/sample_text_file_2.txt', 'DataFrames provide a higher-level abstraction.\\r\\nThis is a sample text file for SparkContext.textFile.')]\n"
     ]
    }
   ],
   "source": [
    "# Each line of file = one record\n",
    "rdd_text = spark.sparkContext.textFile(get_project_path('data', 'sample', 'sample_text_file_1.txt'))\n",
    "print(rdd_text.collect())\n",
    "\n",
    "# Each file = one record\n",
    "rdd_whole = spark.sparkContext.wholeTextFiles(get_project_path('data', 'sample'))\n",
    "print(rdd_whole.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 6: RDD Transformations (Lazy)\n",
    "\n",
    "Transformations define **how to change data**, but do not execute until an action is called.\n",
    "\n",
    "- **distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'is', 'My', 'Darshil']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Darshil']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.filter(lambda w: w.startswith(\"D\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'M', False), ('name', 'n', False), ('is', 'i', False)]\n"
     ]
    }
   ],
   "source": [
    "words2 = words.map(lambda w: (w, w[0], w.startswith(\"D\")))\n",
    "print(words2.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **flatMap** (expands into multiple outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'y', 'n', 'a', 'm']\n"
     ]
    }
   ],
   "source": [
    "chars = words.flatMap(lambda w: list(w))\n",
    "print(chars.take(5))  # ['M','y','n','a','m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sortBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Darshil', 'name']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sortBy(lambda w: len(w), ascending=False).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **randomSplit**<br>\n",
    "This returns an array of RDDs that you can manipulate individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PythonRDD[29] at RDD at PythonRDD.scala:53, PythonRDD[30] at RDD at PythonRDD.scala:53]\n"
     ]
    }
   ],
   "source": [
    "split_rdds = words.randomSplit([0.5, 0.5], seed=42)\n",
    "print(split_rdds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 7: RDD Actions (Eager)\n",
    "\n",
    "Actions **trigger execution** of transformations.\n",
    "\n",
    "- **reduce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "nums = spark.sparkContext.parallelize(range(1, 21))\n",
    "print(nums.reduce(lambda x, y: x + y))  # sum = 210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(words.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n"
     ]
    }
   ],
   "source": [
    "print(words.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max / min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nums.max())  # 20\n",
    "print(nums.min())  # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **take / takeOrdered / top**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Darshil']\n",
      "['Darshil', 'My', 'is', 'name']\n",
      "['name', 'is', 'My', 'Darshil']\n"
     ]
    }
   ],
   "source": [
    "print(words.take(5))\n",
    "print(words.takeOrdered(5))  # lowest sorted\n",
    "print(words.top(5))          # highest sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **countByValue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'My': 1, 'name': 1, 'is': 1, 'Darshil': 1})\n"
     ]
    }
   ],
   "source": [
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **takeSample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'is', 'name']\n"
     ]
    }
   ],
   "source": [
    "sample = words.takeSample(withReplacement=True, num=3, seed=10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 8: Saving RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ‘‰ Delete the existing directory: Before calling saveAsTextFile, you can delete the target directory if it exists.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as text file (each partition -> 1 file)\n",
    "words.saveAsTextFile(\"tmp/words_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 9: Special Functions\n",
    "\n",
    "- **glom()**\n",
    "    - Converts partitions â†’ arrays (useful for debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello'], ['World']]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.parallelize([\"Hello\", \"World\"], 2).glom().collect())\n",
    "# [['Hello'], ['World']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 10: Key Insights\n",
    "\n",
    "- **RDDs = flexible, lower-level control**, but harder to optimize.\n",
    "- **DataFrames = structured, optimized, easier to use**.\n",
    "- Best practice:\n",
    "    - Use **RDDs only when absolutely needed**.\n",
    "    - For everything else, **use DataFrame / Dataset APIs**.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In simple words:**\n",
    "\n",
    "RDDs are like Spark's \"raw ingredients.\" They give you complete control, but you lose automatic optimizations. DataFrames are like \"ready-to-use recipes.\" Spark will optimize them for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
