{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“’ Advanced RDDs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 1: Why Advanced RDDs?\n",
    "\n",
    "- Basic RDDs (`map`, `filter`, `reduce`) work on generic objects.\n",
    "- But **many big data tasks are about grouping and aggregating by keys** (like SQL `GROUP BY`), joining datasets, or controlling partitions for performance.\n",
    "- That's where **Pair RDDs (keyâ€“value RDDs)** come in.\n",
    "\n",
    "ðŸ‘‰ Example:\n",
    "\n",
    "If we want to **count words**, it's natural to turn them into `(word, 1)` pairs and then aggregate by word.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 2: Creating Keyâ€“Value RDDs\n",
    "\n",
    "You can turn any dataset into a keyâ€“value RDD.\n",
    "\n",
    "### Example â€“ Word Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('my', 1), ('name', 1), ('is', 1), ('darshil', 1), ('and', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = spark.sparkContext.parallelize(\n",
    "    \"My Name is Darshil and I love Spark\".split(\" \"), 2\n",
    ")\n",
    "\n",
    "# (word, 1)\n",
    "pairs = words.map(lambda w: (w.lower(), 1))\n",
    "print(pairs.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Explanation:\n",
    "\n",
    "- Each record is now `(key=word, value=1)`.\n",
    "- This makes it possible to use key-based aggregations like `reduceByKey`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 3: Flexible Keys with `keyBy`\n",
    "\n",
    "Instead of manually creating `(key, value)` pairs, we can generate keys dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('m', 'My'), ('n', 'Name'), ('i', 'is'), ('d', 'Darshil'), ('a', 'and')]\n"
     ]
    }
   ],
   "source": [
    "# Key = first letter of word\n",
    "keyword = words.keyBy(lambda w: w.lower()[0])\n",
    "print(keyword.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Explanation:\n",
    "\n",
    "- `keyBy` transforms an RDD into keyâ€“value pairs,\n",
    "- where the key is derived by applying the given function to each element.\n",
    "- Example: `\"Spark\"` â†’ `(\"s\", \"Spark\")`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 4: Working with Keys and Values\n",
    "\n",
    "Once you have keyâ€“value pairs, you can manipulate them more directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('m', 'MY'), ('n', 'NAME'), ('i', 'IS'), ('d', 'DARSHIL'), ('a', 'AND')]\n",
      "['m', 'n', 'i', 'd', 'a']\n",
      "['My', 'Name', 'is', 'Darshil', 'and']\n",
      "['Spark']\n"
     ]
    }
   ],
   "source": [
    "# Convert values to uppercase\n",
    "print(keyword.mapValues(lambda w: w.upper()).take(5))\n",
    "\n",
    "# Extract only keys\n",
    "print(keyword.keys().take(5))\n",
    "\n",
    "# Extract only values\n",
    "print(keyword.values().take(5))\n",
    "\n",
    "# Lookup by key\n",
    "print(keyword.lookup(\"s\"))  # returns ['Spark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Explanation:\n",
    "\n",
    "- `.mapValues()` modifies only the **values** (keeps keys intact).\n",
    "- `.keys()` and `.values()` let you extract just one side.\n",
    "- `.lookup(key)` fetches all values for a given key.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 5: Aggregations\n",
    "\n",
    "### Count characters example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = words.flatMap(lambda w: w.lower())\n",
    "KVchars = chars.map(lambda c: (c, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can aggregate:\n",
    "\n",
    "- **countByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'m': 2, 'y': 1, 'n': 2, 'a': 4, 'e': 2, 'i': 3, 's': 3, 'd': 2, 'r': 2, 'h': 1, 'l': 2, 'o': 1, 'v': 1, 'p': 1, 'k': 1})\n"
     ]
    }
   ],
   "source": [
    "print(KVchars.countByKey())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Returns a dictionary of how many times each key appears.\n",
    "\n",
    "---\n",
    "\n",
    "### groupByKey vs reduceByKey\n",
    "\n",
    "- **groupByKey (âš ï¸ risky for skewed data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('y', 1), ('i', 3), ('s', 3), ('d', 2), ('r', 2)]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "def addFunc(x, y): return x + y\n",
    "\n",
    "print(\n",
    "    KVchars.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1]))).take(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Problem: `groupByKey` brings **all values for a key into memory** â†’ can cause **OutOfMemoryError** if one key has too many values (data skew).\n",
    "\n",
    "---\n",
    "\n",
    "- **reduceByKey (âœ… preferred)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('y', 1), ('i', 3), ('s', 3), ('d', 2), ('r', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(KVchars.reduceByKey(addFunc).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Why better?\n",
    "\n",
    "- Combines values **within each partition first** before shuffling.\n",
    "- Less data movement â†’ faster & safer.\n",
    "\n",
    "ðŸ‘‰ Rule of thumb:\n",
    "\n",
    "- Use `reduceByKey` for additive/mergeable operations.\n",
    "- Use `groupByKey` only if you really need all values together (rare).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 6: Joins\n",
    "\n",
    "You can join two keyâ€“value RDDs just like SQL joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('m', (1, 0.22564860562641875)), ('m', (1, 0.22564860562641875)), ('y', (1, 0.8805029896320028)), ('n', (1, 0.30812983281156814)), ('n', (1, 0.30812983281156814))]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# distinct letters keyed with random numbers\n",
    "distinctChars = chars.distinct()\n",
    "keyedChars = distinctChars.map(lambda c: (c, random.random()))\n",
    "\n",
    "# Inner join\n",
    "print(KVchars.join(keyedChars, 10).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Explanation:\n",
    "\n",
    "- `join` combines records with the same key.\n",
    "- The second argument (`10`) specifies number of output partitions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 7: Controlling Partitions\n",
    "\n",
    "Partitioning = **how Spark splits data across executors**.\n",
    "\n",
    "- **coalesce** â†’ reduce partitions without shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(words.coalesce(1).getNumPartitions())  # collapses to 1 partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **repartition** â†’ change partitions with shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(words.repartition(10).getNumPartitions())  # now 10 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Explanation:\n",
    "\n",
    "- Use `coalesce` to shrink partitions cheaply.\n",
    "- Use `repartition` when you need more parallelism (adds shuffle overhead).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 8: Custom Partitioning\n",
    "\n",
    "Sometimes, default partitioning causes **data skew** (one key gets too much data).\n",
    "\n",
    "Custom partitioning helps you **control data distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(InvoiceNo='10001', StockCode='A123', Description='Apple', Quantity=5, UnitPrice=10.0, CustomerID=17850, Country='UK')\n",
      "Row length: 7\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"10001\", \"A123\", \"Apple\",     5, 10.0, 17850, \"UK\"),\n",
    "    (\"10002\", \"B456\", \"Banana\",   10,  5.5, 12583, \"UK\"),\n",
    "    (\"10003\", \"C789\", \"Carrot\",    2,  2.0, 17850, \"France\"),\n",
    "    (\"10004\", \"D111\", \"Dates\",     1, 20.0, 11111, \"USA\"),\n",
    "]\n",
    "\n",
    "columns = [\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"UnitPrice\", \"CustomerID\", \"Country\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "rdd = df.rdd\n",
    "first_row = rdd.first()\n",
    "print(first_row)\n",
    "print(\"Row length:\", len(first_row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "keyedRDD = rdd.keyBy(lambda row: row.CustomerID)\n",
    "\n",
    "def partitionFunc(key):\n",
    "    import random\n",
    "    if key in [17850, 12583]:  # skewed customers\n",
    "        return 0\n",
    "    else:\n",
    "        return random.randint(1, 2)\n",
    "\n",
    "partitioned = keyedRDD.partitionBy(3, partitionFunc)\n",
    "print(partitioned.glom().map(len).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Explanation:\n",
    "\n",
    "- Custom partitioning is only possible with RDDs (not DataFrames).\n",
    "- Here, we isolate heavy keys into their own partition â†’ avoids bottlenecks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Step 9: Key Takeaways\n",
    "\n",
    "1. **Keyâ€“Value RDDs** allow aggregations and joins like SQL.\n",
    "2. **reduceByKey > groupByKey** for performance.\n",
    "3. **Partitioning control** is the biggest reason to still use RDDs.\n",
    "4. Custom partitioners help fight **data skew** in large-scale jobs.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In simple words:**\n",
    "\n",
    "Advanced RDDs let you treat data as `(key, value)` pairs, which unlocks aggregations and joins. The real superpower is **custom partitioning** â€” you can decide how data is spread across the cluster to avoid skew. This level of control is why RDDs still matter, even though DataFrames should be your default choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
