{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fa0995",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "217eab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f263c",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b493f28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b69324",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e788777",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a3f695c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad9826",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "482cf377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1597705",
   "metadata": {},
   "source": [
    "Import pyspark package and create a spark sesstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6fa839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDataExample\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f16e2",
   "metadata": {},
   "source": [
    "# Create Dataframe (Load Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b44579",
   "metadata": {},
   "source": [
    "üîπ 1. Creating a DataFrame by reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0e795d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "flight_data_path_2015 = get_project_path(\"data\", \"darshil-data\", \"flight-data\", \"json\", \"2015-summary.json\")\n",
    "\n",
    "df = spark.read.format(\"json\").load(flight_data_path_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead996e",
   "metadata": {},
   "source": [
    "- Spark reads the JSON file and infers the schema automatically.\n",
    "\n",
    "- This is the most common way of creating DataFrames in real projects (from files, tables, databases, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277016c",
   "metadata": {},
   "source": [
    "**You then registered it as a temporary SQL view:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8f85874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f97d6",
   "metadata": {},
   "source": [
    "Now you can query it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "903e9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|   15|\n",
      "|    United States|    1|\n",
      "|    United States|  344|\n",
      "|            Egypt|   15|\n",
      "|    United States|   62|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, count FROM dfTable LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d51dda",
   "metadata": {},
   "source": [
    "üîπ 2. Creating a DataFrame manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b119ad",
   "metadata": {},
   "source": [
    "This is useful when you want full control over schema (columns + datatypes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2df7f3",
   "metadata": {},
   "source": [
    "*Step 1: Define schema*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bd413e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"name\", StringType(), True),   # nullable = True\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), False)    # nullable = False\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa432996",
   "metadata": {},
   "source": [
    "- StructType = the schema (table structure).\n",
    "\n",
    "- StructField = each column: name, datatype, nullable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc1fd9",
   "metadata": {},
   "source": [
    "*Step 2: Create a Row*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e319c0",
   "metadata": {},
   "source": [
    "üîπ 1. What is a Row?\n",
    "\n",
    "- A Row is Spark‚Äôs way of representing a single record (like one line in a spreadsheet or a table).\n",
    "\n",
    "- A DataFrame = collection of Rows + schema.\n",
    "\n",
    "- A Row by itself does not have column names ‚Üí just positional values.\n",
    "\n",
    "Example from your DataFrame (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "825b7a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a16cb",
   "metadata": {},
   "source": [
    "Each of these lines is internally a Row object:\n",
    "\n",
    "*Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51023b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myRow = Row(\"Vinay\", None, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320dec72",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Order matters here ‚Üí the values must match schema order:\n",
    "\n",
    "- \"Vinay\" ‚Üí goes to column \"name\"\n",
    "\n",
    "- None ‚Üí goes to \"city\"\n",
    "\n",
    "- 30 ‚Üí goes to \"age\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f4c34",
   "metadata": {},
   "source": [
    "**Accessing Rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc3fd7",
   "metadata": {},
   "source": [
    "By index (positional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d91a1eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vinay\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(myRow[0])   # \"Vinay\"\n",
    "print(myRow[2])   # 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae771f",
   "metadata": {},
   "source": [
    "By attribute name (only if Row was created with named fields):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b8a886f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vinay\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "p1 = Person(\"Vinay\", 28)\n",
    "\n",
    "print(p1.name)   # \"Vinay\"\n",
    "print(p1.age)    # 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe01274",
   "metadata": {},
   "source": [
    "*Step 3: Create DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "579e236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+\n",
      "| name|city|age|\n",
      "+-----+----+---+\n",
      "|Vinay|NULL| 30|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76a9ee",
   "metadata": {},
   "source": [
    "üîπ 3. Difference between the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca15d2e",
   "metadata": {},
   "source": [
    "| Method                               | When to use                                                                                                                           |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Read from file (inferred schema)** | Most common ‚Äî fast, Spark figures out schema itself.                                                                                  |\n",
    "| **Manual schema + Rows**             | When: <br>‚Ä¢ You‚Äôre creating test DataFrames <br>‚Ä¢ You want strict schema (avoid inference mistakes) <br>‚Ä¢ You‚Äôre combining with RDDs. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ee792",
   "metadata": {},
   "source": [
    "‚úÖ Key takeaways:\n",
    "\n",
    "- Row = record (like a tuple).\n",
    "\n",
    "- When manually creating Rows, order matters because there‚Äôs no schema.\n",
    "\n",
    "- Use Row(\"col1\", \"col2\") style if you want name-based access.\n",
    "\n",
    "- DataFrames = schema + collection of Rows.\n",
    "\n",
    "- You can either read data (auto schema) or build manually (defined schema).\n",
    "\n",
    "- Manual schemas are especially useful in testing & ensuring data types match exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d30c5",
   "metadata": {},
   "source": [
    "# Columns and Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3a176",
   "metadata": {},
   "source": [
    "**Columns in Spark are similar to columns in a spreadsheet or pandas DataFrame. You can select, manipulate, and remove columns from DataFrames and these operations are represented as expressions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2661bb",
   "metadata": {},
   "source": [
    "üîπ 1. Using col and column\n",
    "\n",
    "Both col and column do the same thing ‚Äî they create a column expression by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a6e13955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "# Both are equivalent\n",
    "df.select(col(\"count\")).show(5)\n",
    "df.select(column(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d337c",
   "metadata": {},
   "source": [
    "üîπ 2. Using expr for expressions\n",
    "\n",
    "An expression is just a SQL string translated into a column object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "73762147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# expr is same as col in simple case\n",
    "df.select(expr(\"count\")).show(5)  \n",
    "df.select(col(\"count\")).show(5)    # both identical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b5a6b",
   "metadata": {},
   "source": [
    "But expr becomes powerful when chaining logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5609ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count_plus_10|\n",
      "+-------------+\n",
      "|           25|\n",
      "|           11|\n",
      "|          354|\n",
      "|           25|\n",
      "|           72|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|        Netherlands|  660|\n",
      "|    United States|         Costa Rica|  608|\n",
      "|    United States|            Jamaica|  712|\n",
      "|    United States|        The Bahamas|  986|\n",
      "|    United States|              China|  920|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+\n",
      "|is_small|\n",
      "+--------+\n",
      "|    true|\n",
      "|    true|\n",
      "|    true|\n",
      "|    true|\n",
      "|    true|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic expression\n",
    "df.select(expr(\"count + 10 as count_plus_10\")).show(5)\n",
    "\n",
    "# Logical expression\n",
    "df.filter(expr(\"count > 500 AND DEST_COUNTRY_NAME = 'United States'\")).show(5)\n",
    "\n",
    "# Complex nested expression\n",
    "df.select(expr(\"(((count + 5) * 2) - 6) < 10000 as is_small\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a4408",
   "metadata": {},
   "source": [
    "‚úÖ Key takeaway:\n",
    "\n",
    "- col(\"x\") and expr(\"x\") both represent a column expression.\n",
    "\n",
    "- Expressions are lazy recipes, not values. Spark builds a plan with them and executes later.\n",
    "\n",
    "- Use col when writing Pythonic transformations, and expr when SQL syntax is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10baf605",
   "metadata": {},
   "source": [
    "Accessing a DataFrame‚Äôs columns to see all columns on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ad386cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ce2f8",
   "metadata": {},
   "source": [
    "# Spark Transformations: ``` select()```, ```expr()```, and ```selectExpr()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95eb65",
   "metadata": {},
   "source": [
    "üîπ 1. ```select``` (DataFrame-style, column-based)\n",
    "\n",
    "This is like saying ‚Äúgive me these columns‚Äù.\n",
    "\n",
    "Equivalent SQL:\n",
    "\n",
    "```SQL\n",
    "SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME\n",
    "FROM dfTable\n",
    "LIMIT 2;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1ad5eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One column\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "# Multiple columns\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffe8a7",
   "metadata": {},
   "source": [
    "üîπ 2. Using ```col```, ```column```, and ```expr``` inside ```select```\n",
    "\n",
    "Spark lets you build column expressions instead of just plain names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f3459e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),     # SQL-style\n",
    "    col(\"DEST_COUNTRY_NAME\"),      # Python-friendly\n",
    "    column(\"DEST_COUNTRY_NAME\")    # same as col\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0b3f6",
   "metadata": {},
   "source": [
    "üîπ 3. Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c54d83",
   "metadata": {},
   "source": [
    "- With SQL ```AS```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f8b3cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa35b05",
   "metadata": {},
   "source": [
    "- Or with ```.alias()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f23b1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\").alias(\"destination\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a65c40",
   "metadata": {},
   "source": [
    "- Even combining both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f526ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccc1e2",
   "metadata": {},
   "source": [
    "üîπ 4. ```selectExpr``` (SQL-style in DataFrames)\n",
    "\n",
    "```selectExpr``` is a shortcut for when you want to use SQL expressions directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6cbc8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858a061",
   "metadata": {},
   "source": [
    "üëâ Same as writing ```spark.sql(\"SELECT DEST_COUNTRY_NAME as newColumnName, DEST_COUNTRY_NAME FROM dfTable\")```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63f949",
   "metadata": {},
   "source": [
    "üîπ 5. Add new derived columns with ```selectExpr```\n",
    "\n",
    "You can write expressions directly in SQL style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0ff5866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"*\",   # keep all existing columns\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\"  # new boolean col\n",
    ").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256abaa",
   "metadata": {},
   "source": [
    "üëâ This adds a new column ```withinCountry``` that checks if ```DEST_COUNTRY_NAME``` equals ```ORIGIN_COUNTRY_NAME```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15753eda",
   "metadata": {},
   "source": [
    "üîπ 6. Aggregations inside ```selectExpr```\n",
    "\n",
    "You can run aggregate functions directly:\n",
    "\n",
    "Equivalent SQL:\n",
    "\n",
    "```SQL\n",
    "SELECT avg(count), count(distinct(DEST_COUNTRY_NAME))\n",
    "FROM dfTable;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9ce586db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+\n",
      "|  avg_count|distinct_country_count|\n",
      "+-----------+----------------------+\n",
      "|1770.765625|                   132|\n",
      "+-----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count) as avg_count\", \"count(distinct(DEST_COUNTRY_NAME)) as distinct_country_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ea400",
   "metadata": {},
   "source": [
    "‚úÖ Key takeaways:\n",
    "\n",
    "- ```select``` ‚Üí Pythonic, works well with ```col``` & functions.\n",
    "\n",
    "- `expr` ‚Üí lets you write SQL inside `select`.\n",
    "\n",
    "- `selectExpr` ‚Üí shortcut for writing multiple SQL expressions in one go.\n",
    "\n",
    "- If you know SQL, `selectExpr` feels very natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fac532",
   "metadata": {},
   "source": [
    "# Spark Column Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca206886",
   "metadata": {},
   "source": [
    "üîπ 1. Converting to Spark Types (Literals)\n",
    "\n",
    "Sometimes you want to insert a constant value (not from a column) into your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c5590c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, expr\n",
    "\n",
    "# Add a constant column with value 1\n",
    "df.select(expr(\"*\"), lit(1).alias(\"numberOne\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe97cd",
   "metadata": {},
   "source": [
    "üëâ `lit(1)` means \"literal 1\" ‚Üí Spark knows it's not a column, just a fixed value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676c0df",
   "metadata": {},
   "source": [
    "üîπ 2. Adding Columns (`withColumn`)\n",
    "\n",
    "The formal way to add or replace a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aaf33523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column with a constant\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "# Add a new column with an expression\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee6754",
   "metadata": {},
   "source": [
    "üëâ `withColumn(name, expression)`\n",
    "\n",
    "`name` = new/existing column name\n",
    "\n",
    "`expression` = column transformation / literal / condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8f96d",
   "metadata": {},
   "source": [
    "üîπ 3. Renaming Columns\n",
    "\n",
    "Rename one column at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d4e51dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e911b2",
   "metadata": {},
   "source": [
    "üîπ 4. Removing Columns (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9aeb6a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop one column\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns\n",
    "\n",
    "# Drop multiple columns\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89a4c2",
   "metadata": {},
   "source": [
    "üîπ 5. Changing Column Type (Casting)\n",
    "\n",
    "Convert one column‚Äôs data type to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c2eb2f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- count2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395201a",
   "metadata": {},
   "source": [
    "üëâ Useful when Spark reads data as `string` but you need it as `integer/long/float`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8dc23",
   "metadata": {},
   "source": [
    "üîπ 6. Putting it all together (mini example)\n",
    "\n",
    "Imagine you want:\n",
    "\n",
    "- Keep all columns\n",
    "\n",
    "- Add a constant column\n",
    "\n",
    "- Add a boolean flag\n",
    "\n",
    "- Rename a column\n",
    "\n",
    "- Drop another column\n",
    "\n",
    "- Cast a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eedad735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+-------------+----------+\n",
      "|         dest|count|numberOne|withinCountry|count_long|\n",
      "+-------------+-----+---------+-------------+----------+\n",
      "|United States|   15|        1|        false|        15|\n",
      "|United States|    1|        1|        false|         1|\n",
      "|United States|  344|        1|        false|       344|\n",
      "|        Egypt|   15|        1|        false|        15|\n",
      "|United States|   62|        1|        false|        62|\n",
      "+-------------+-----+---------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- dest: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- numberOne: integer (nullable = false)\n",
      " |-- withinCountry: boolean (nullable = true)\n",
      " |-- count_long: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = (df\n",
    "    .withColumn(\"numberOne\", lit(1))   # constant col\n",
    "    .withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))  # boolean col\n",
    "    .withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\")  # rename\n",
    "    .drop(\"ORIGIN_COUNTRY_NAME\")  # remove col\n",
    "    .withColumn(\"count_long\", col(\"count\").cast(\"long\"))  # cast\n",
    ")\n",
    "\n",
    "df_transformed.show(5)\n",
    "df_transformed.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37fd29",
   "metadata": {},
   "source": [
    "‚úÖ Key takeaways:\n",
    "\n",
    "- `lit()` ‚Üí add constants\n",
    "\n",
    "- `withColumn()` ‚Üí add or overwrite columns\n",
    "\n",
    "- `withColumnRenamed()` ‚Üí rename columns\n",
    "\n",
    "- `drop()` ‚Üí remove columns\n",
    "\n",
    "- `cast()` ‚Üí change column types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e11a9",
   "metadata": {},
   "source": [
    "# Spark Row Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4d847",
   "metadata": {},
   "source": [
    "üîπ 1. Filtering Rows\n",
    "\n",
    "You can filter using:\n",
    "\n",
    "- `filter` (Pythonic, column expressions)\n",
    "\n",
    "- `where` (SQL-style, string expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f0ac671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter with col\n",
    "df.filter(col(\"count\") < 2).show(2)\n",
    "\n",
    "# Same with where (SQL-style string)\n",
    "df.where(\"count < 2\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f727504",
   "metadata": {},
   "source": [
    "üëâ Both are equivalent. <br>\n",
    "üëâ Use col(...) if you want Python code completion, use strings if you prefer SQL style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165670e1",
   "metadata": {},
   "source": [
    "**Multiple Filters**\n",
    "\n",
    "You can chain multiple conditions:\n",
    "\n",
    "Equivalent in SQL:\n",
    "```SQL\n",
    "SELECT *\n",
    "FROM dfTable\n",
    "WHERE count < 2 AND ORIGIN_COUNTRY_NAME != 'Croatia'\n",
    "LIMIT 2;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "097fe359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2) \\\n",
    "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\") \\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea1d15",
   "metadata": {},
   "source": [
    "üîπ 2. Getting Unique Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e3ddf",
   "metadata": {},
   "source": [
    "üëâ This counts unique pairs of (ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME).<br>\n",
    "üëâ distinct() works like SELECT DISTINCT in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fc5b5931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8198b",
   "metadata": {},
   "source": [
    "üîπ 3. Concatenating & Appending Rows (Union)\n",
    "\n",
    "To add new rows to a DataFrame ‚Üí use `union`.\n",
    "‚ö†Ô∏è Both DataFrames must have the same schema (column names + types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "013d3e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "schema = df.schema  # reuse schema\n",
    "\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),   # note: plain int works\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "# Append and filter\n",
    "df.union(newDF) \\\n",
    "  .where(\"count = 1\") \\\n",
    "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4293401",
   "metadata": {},
   "source": [
    "üîπ 4. Sorting Rows\n",
    "\n",
    "Two equivalent methods:\n",
    "\n",
    "- `sort`\n",
    "\n",
    "- `orderBy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "379a615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "# Sort by one column\n",
    "df.sort(\"count\").show(5)\n",
    "\n",
    "# Sort by multiple columns\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "\n",
    "# Explicit with col\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)\n",
    "\n",
    "# üëâ To specify ascending/descending explicitly:\n",
    "\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444c093",
   "metadata": {},
   "source": [
    "üîπ 5. Limiting Rows\n",
    "\n",
    "Like SQL `LIMIT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "957139ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "\n",
    "# Top 6 largest counts\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5a2eb",
   "metadata": {},
   "source": [
    "‚úÖ Quick Recap\n",
    "\n",
    "- `filter / where` ‚Üí keep rows that satisfy conditions.\n",
    "\n",
    "- `distinct()` ‚Üí unique rows.\n",
    "\n",
    "- `union()` ‚Üí append DataFrames (schema must match).\n",
    "\n",
    "- `sort() / orderBy()` ‚Üí order rows.\n",
    "\n",
    "- `limit()` ‚Üí restrict number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261cc73",
   "metadata": {},
   "source": [
    "# Spark Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb1491",
   "metadata": {},
   "source": [
    "## üîπ 1. Partitions in Spark\n",
    "- Spark breaks up your DataFrame into partitions (chunks of data distributed across executors).\n",
    "\n",
    "- By default, when you load small files, you often get 1 partition.\n",
    "\n",
    "- More partitions = more parallelism (but too many small partitions = overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ccf9fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()  \n",
    "# often 1 for small JSON/CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108704f",
   "metadata": {},
   "source": [
    "##üîπ 2. Repartition\n",
    "\n",
    "- repartition(n) ‚Üí creates exactly n partitions.\n",
    "\n",
    "- Always triggers a full shuffle (data is redistributed across executors).\n",
    "\n",
    "- Useful when:\n",
    "\n",
    "    - You need more parallelism (increase partitions).\n",
    "\n",
    "    - You want to partition by column for better query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "07a79441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Repartition into 5 partitions\n",
    "df2 = df.repartition(5)\n",
    "print(df2.rdd.getNumPartitions())\n",
    "\n",
    "# Repartition based on column (common filter key)\n",
    "df3 = df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "\n",
    "# Repartition into 5 partitions *by column*\n",
    "df4 = df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e4543",
   "metadata": {},
   "source": [
    "üëâ If you often filter by `DEST_COUNTRY_NAME`, this ensures rows with the same country end up in the same partition ‚Üí less shuffling later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48e2ad",
   "metadata": {},
   "source": [
    "## üîπ 3. Coalesce\n",
    "\n",
    "- `coalesce(n)` ‚Üí reduces the number of partitions without a full shuffle (faster).\n",
    "\n",
    "- It merges partitions together instead of redistributing.\n",
    "\n",
    "- Useful when you need fewer partitions, e.g., before writing out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0201980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Start with 5 partitions, reduce to 2\n",
    "df5 = df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\n",
    "print(df5.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e263fe",
   "metadata": {},
   "source": [
    "üëâ Rule of thumb:\n",
    "\n",
    "- Need more partitions or shuffle by column ‚Üí use `repartition`.\n",
    "\n",
    "- Need fewer partitions ‚Üí use `coalesce`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf2630",
   "metadata": {},
   "source": [
    "## üîπ 4. Collecting Rows\n",
    "\n",
    "Collecting moves data from executors ‚Üí driver program.<br> **‚ö†Ô∏è Dangerous for large datasets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "93d4ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344), Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62), Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588), Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40), Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]\n"
     ]
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "\n",
    "# Get first N rows as list of Row objects\n",
    "collectDF.take(5)    \n",
    "\n",
    "# Print nicely in tabular form\n",
    "collectDF.show()     \n",
    "collectDF.show(5, False)  # don't truncate\n",
    "\n",
    "# Collect all rows into Python list (‚ö†Ô∏è expensive!)\n",
    "rows = collectDF.collect()\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1a073",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Important warnings:**\n",
    "\n",
    "- `collect()` brings *all data* to the driver. If your dataset is big, this can crash the driver.\n",
    "\n",
    "- `take(n)` is safer ‚Üí only retrieves `n` `rows`.\n",
    "\n",
    "- `toLocalIterator()` is very expensive ‚Üí retrieves row by row to driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2553f",
   "metadata": {},
   "source": [
    "## **‚úÖ Quick Recap**\n",
    "\n",
    "- `repartition(n)` ‚Üí increase partitions, full shuffle.\n",
    "\n",
    "- `repartition(n, col)` ‚Üí distribute by column, shuffle.\n",
    "\n",
    "- `coalesce(n)` ‚Üí decrease partitions, no full shuffle.\n",
    "\n",
    "- `collect()` ‚Üí brings all data to driver (**‚ö†Ô∏è use carefully**).\n",
    "\n",
    "- `take(n)` / `show(n)` ‚Üí safer, preview data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
