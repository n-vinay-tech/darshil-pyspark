{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba8557e",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110e8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f7434",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6aa278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a3577",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e5837",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be77715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e68787",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b4628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94609166",
   "metadata": {},
   "source": [
    "Import pyspark package and create a spark sesstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4e252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDataExample\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690951bc",
   "metadata": {},
   "source": [
    "# üìí Section 1: Reading a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bd80d",
   "metadata": {},
   "source": [
    "### üîé Step 1: What does this code do?\n",
    "\n",
    "1. **`spark.read`**\n",
    "    - Uses the active **SparkSession** (`spark`) to create a DataFrameReader.\n",
    "    - Think of it as: *‚ÄúI want Spark to read some data.‚Äù*\n",
    "2. **`.format(\"csv\")`**\n",
    "    - Tells Spark the file format is **CSV**.\n",
    "    - Other formats supported: `\"json\"`, `\"parquet\"`, `\"orc\"`, `\"jdbc\"`, etc.\n",
    "    - Default (if omitted) = `\"parquet\"`.\n",
    "3. **`.option(\"header\", \"true\")`**\n",
    "    - Indicates the first row of the CSV file contains column names.\n",
    "    - If `\"false\"`, Spark will assign generic column names like `_c0, _c1, _c2...`.\n",
    "4. **`.option(\"inferSchema\", \"true\")`**\n",
    "    - Spark will automatically detect column data types (string, integer, double, etc.).\n",
    "    - If `\"false\"`, all columns are read as **string** by default.\n",
    "5. **`.load(\"path\")`**\n",
    "    - Specifies the location of the data file.\n",
    "    - Here, it‚Äôs: `\"/data/retail-data/by-day/2010-12-01.csv\"`.\n",
    "    - Spark reads this file and returns a **DataFrame**.\n",
    "6. **`df`**\n",
    "    - The DataFrame that holds your data.\n",
    "    - Think of it like a **distributed table**: each row = record, each column = field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cccf21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = get_project_path('data', 'darshil-data', 'retail-data', 'by-day', '2010-12-01.csv')\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(data_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69744a53",
   "metadata": {},
   "source": [
    "### üîé Step 2: Inspecting the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca6a31",
   "metadata": {},
   "source": [
    "üëâ View schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c8cbffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004694f1",
   "metadata": {},
   "source": [
    "üëâ Show sample rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5838ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27435cf",
   "metadata": {},
   "source": [
    "üëâ Register as SQL table for queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cbaef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240fb63",
   "metadata": {},
   "source": [
    "Now you can query from temp view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa338d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dfTable LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799f386",
   "metadata": {},
   "source": [
    "# üìí Section 2: Literals in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c2716",
   "metadata": {},
   "source": [
    "### üîé Step 1: What does this code do?\n",
    "\n",
    "1. **`lit()` function**\n",
    "    - Comes from `pyspark.sql.functions`.\n",
    "    - It creates a **literal column** in Spark (i.e., a constant value treated as a column).\n",
    "2. **Why is it needed?**\n",
    "    - Spark DataFrames operate on **columns** (not raw Python values).\n",
    "    - If you want to add a constant value (e.g., `5`) to every row, you need to wrap it inside `lit()`.\n",
    "3. **In this example:**\n",
    "    - `lit(5)` ‚Üí creates a constant integer column with value `5`.\n",
    "    - `lit(\"five\")` ‚Üí creates a constant string column `\"five\"`.\n",
    "    - `lit(5.0)` ‚Üí creates a constant float column `5.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34948c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59defbf0",
   "metadata": {},
   "source": [
    "### üîé Step 2: Example outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da317e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+\n",
      "|int_col|string_col|float_col|\n",
      "+-------+----------+---------+\n",
      "|      5|      five|      5.0|\n",
      "|      5|      five|      5.0|\n",
      "|      5|      five|      5.0|\n",
      "|      5|      five|      5.0|\n",
      "|      5|      five|      5.0|\n",
      "+-------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(5).alias(\"int_col\"),\n",
    "          lit(\"five\").alias(\"string_col\"),\n",
    "          lit(5.0).alias(\"float_col\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0094f",
   "metadata": {},
   "source": [
    "**Notice**: all rows have the same constant values.\n",
    "\n",
    "**These are now `Spark columns (not Python variables)`, so you can use them in transformations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dabb5",
   "metadata": {},
   "source": [
    "### üîé Step 3: Why is this important?\n",
    "\n",
    "- Literals let you:\n",
    "    - Add **constant columns** for tagging data (`lit(\"2025\")` for year, etc.).\n",
    "    - Use constants inside expressions (`col(\"Quantity\") + lit(10)`).\n",
    "    - Make DataFrames behave more like SQL (where constants are allowed in queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0635f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|adjustedQuantity|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|              16|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|              16|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|              18|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|              16|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|              16|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"adjustedQuantity\", df.Quantity + lit(10)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818088ee",
   "metadata": {},
   "source": [
    "‚úÖ In simple words:<br>\n",
    "`lit()` lets you insert fixed values into a DataFrame as if they were columns. It‚Äôs the bridge between normal Python constants and Spark DataFrame columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66712fe6",
   "metadata": {},
   "source": [
    "# üìí Section 3: Working with Booleans\n",
    "\n",
    "`Booleans (True / False)` are the foundation of filtering in Spark.\n",
    "We use them in expressions to decide which rows to keep or discard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfec5c",
   "metadata": {},
   "source": [
    "### üîé Step 1: Filtering with `col`\n",
    "\n",
    "We can build conditions using `col` from `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f798c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.where(col(\"InvoiceNo\") != 536365) \\\n",
    "  .select(\"InvoiceNo\", \"Description\") \\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd4ee2",
   "metadata": {},
   "source": [
    "üëâ Explanation:\n",
    "\n",
    "- `col(\"InvoiceNo\") != 536365` ‚Üí creates a Boolean expression (`True`/`False`) for each row.\n",
    "- `where(...)` (or `.filter(...)`) keeps only rows where the condition is `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92e8f9",
   "metadata": {},
   "source": [
    "### üîé Step 2: Filtering with SQL-style strings\n",
    "\n",
    "Instead of using `col`, you can directly write the condition as a string expression (like SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6874f0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\").show(5, False)\n",
    "\n",
    "df.where(\"InvoiceNo <> 536365\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8152e72",
   "metadata": {},
   "source": [
    "### üîé Step 3: Combining Boolean Expressions\n",
    "\n",
    "We can use `and`, `or` logic.\n",
    "\n",
    "‚ö†Ô∏è In Spark, you can‚Äôt use plain Python `and` / `or` inside `col` expressions. Instead, you use `&` (and), `|` (or).\n",
    "\n",
    "But chaining `.where()` calls is often the **cleanest way**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ddd1250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "\n",
    "df.where(df.StockCode.isin(\"DOT\")) \\\n",
    "  .where(priceFilter | descripFilter) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17de76c",
   "metadata": {},
   "source": [
    "üëâ Explanation:\n",
    "\n",
    "- `isin(\"DOT\")` ‚Üí checks membership (like SQL `IN`).\n",
    "- `instr(df.Description, \"POSTAGE\") >= 1` ‚Üí checks if the word ‚ÄúPOSTAGE‚Äù exists in the `Description` column.\n",
    "\n",
    "Equivalent SQL:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM dfTable\n",
    "WHERE StockCode IN (\"DOT\")\n",
    "  AND (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1);\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4a1a7",
   "metadata": {},
   "source": [
    "### üîé Step 4: Why is this important?\n",
    "\n",
    "- Booleans let you **filter data** ‚Üí one of the most common tasks.\n",
    "- You can combine multiple conditions to replicate complex SQL `WHERE` clauses.\n",
    "- Spark optimizes these Boolean filters for performance, even across distributed data.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Boolean expressions (`=, !=, >, isin, instr`) let you build filters to pick only the rows you care about. Think of them as Spark‚Äôs version of SQL `WHERE` clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8736431",
   "metadata": {},
   "source": [
    "# üìí Section 4: Working with Numbers\n",
    "\n",
    "When analyzing data, numerical operations are everywhere ‚Äî sums, multiplications, powers, rounding, etc.\n",
    "In Spark, you can directly use columns in arithmetic expressions just like variables in math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d406a9",
   "metadata": {},
   "source": [
    "### üîé Step 1: Arithmetic Expressions\n",
    "\n",
    "Let‚Äôs say we want to create a new quantity:\n",
    "\n",
    "$$\\text{realQuantity} = (Quantity \\times UnitPrice)^2 + 5$$\n",
    "\n",
    "We can express this using both **Pythonic expressions** and **SQL-style expressions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5d11c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, pow\n",
    "\n",
    "# Pythonic way\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)\n",
    "\n",
    "# SQL-style expression\n",
    "df.selectExpr(\"CustomerId\", \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e006d",
   "metadata": {},
   "source": [
    "### üîé Step 2: Rounding\n",
    "\n",
    "Sometimes you need to **round numbers** for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7cfd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af70bcc3",
   "metadata": {},
   "source": [
    "üëâ Explanation:\n",
    "\n",
    "- `round()` ‚Üí standard rounding (2.5 ‚Üí 3).\n",
    "- `bround()` ‚Üí **banker‚Äôs rounding** (rounds 2.5 ‚Üí 2, 3.5 ‚Üí 4).\n",
    "\n",
    "This matters in financial datasets where precise rounding rules apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a69ab",
   "metadata": {},
   "source": [
    "### üîé Step 3: Why is this important?\n",
    "\n",
    "- Arithmetic with columns lets you **derive new metrics** (like revenue = price √ó quantity).\n",
    "- `pow` and `expr` allow complex formulas.\n",
    "- Rounding ensures **clean reporting** and avoids floating-point quirks.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Spark treats numeric columns like math variables. You can multiply, add, take powers, and round them, just like in Excel or SQL ‚Äî but at **distributed scale**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf242c",
   "metadata": {},
   "source": [
    "# üìí Section 5: Working with Strings\n",
    "\n",
    "String manipulation is super common in data analysis:\n",
    "\n",
    "- Cleaning messy text\n",
    "- Formatting case (upper/lower)\n",
    "- Padding or trimming spaces\n",
    "- Replacing characters\n",
    "- Checking if a substring exists\n",
    "\n",
    "Spark provides many built-in functions for string handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4c63e",
   "metadata": {},
   "source": [
    "### üîé Step 1: Capitalization\n",
    "\n",
    "Convert strings into **Title Case** (first letter capitalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2832ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap, col\n",
    "\n",
    "df.select(initcap(col(\"Description\"))).show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3aeb4",
   "metadata": {},
   "source": [
    "### üîé Step 2: Uppercase & Lowercase\n",
    "\n",
    "Convert strings fully to **upper** or **lower** case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e95e3517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|Description                        |lowercase                          |upper_after_lower                  |\n",
      "+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |white hanging heart t-light holder |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE METAL LANTERN                |white metal lantern                |WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |cream cupid hearts coat hanger     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|knitted union flag hot water bottle|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |red woolly hottie white heart.     |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    lower(col(\"Description\")).alias(\"lowercase\"),\n",
    "    upper(lower(col(\"Description\"))).alias(\"upper_after_lower\")\n",
    ").show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e05122",
   "metadata": {},
   "source": [
    "### üîé Step 3: Trimming & Padding\n",
    "\n",
    "Remove spaces, or add padding to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "255046fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+----------+----------+\n",
      "|ltrim   |rtrim   |trim |lpad      |rpad      |\n",
      "+--------+--------+-----+----------+----------+\n",
      "|HELLO   |   HELLO|HELLO|-----HELLO|HELLO.....|\n",
      "|HELLO   |   HELLO|HELLO|-----HELLO|HELLO.....|\n",
      "+--------+--------+-----+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO   \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 10, \"-\").alias(\"lpad\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \".\").alias(\"rpad\")\n",
    ").show(2, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26abd2",
   "metadata": {},
   "source": [
    "üëâ Explanation:\n",
    "\n",
    "- `ltrim` ‚Üí removes spaces from left.\n",
    "- `rtrim` ‚Üí removes spaces from right.\n",
    "- `trim` ‚Üí removes spaces from both sides.\n",
    "- `lpad(\"HELLO\", 10, \"-\")` ‚Üí `\"-----HELLO\"`.\n",
    "- `rpad(\"HELLO\", 10, \".\")` ‚Üí `\"HELLO.....\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69615d28",
   "metadata": {},
   "source": [
    "### üîé Step 4: Replace/Translate Characters\n",
    "\n",
    "Replace multiple characters at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c6db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|leet_text                          |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHI73 M37A1 1AN73RN                |WHITE METAL LANTERN                |\n",
      "|CR3AM CUPID H3AR7S COA7 HANG3R     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNI773D UNION F1AG HO7 WA73R BO7713|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|R3D WOO11Y HO77I3 WHI73 H3AR7.     |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.select(\n",
    "    translate(col(\"Description\"), \"LEET\", \"1337\").alias(\"leet_text\"),\n",
    "    col(\"Description\")\n",
    ").show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076bf98",
   "metadata": {},
   "source": [
    "‚úÖ In simple words:\n",
    "String functions let you clean and format text ‚Äî trimming spaces, changing case, replacing values ‚Äî which is crucial for making messy data usable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e800e6f",
   "metadata": {},
   "source": [
    "# üìí Section 6: Working with Dates & Timestamps\n",
    "\n",
    "Dates and times are tricky in programming. Spark provides built-in functions to:\n",
    "\n",
    "- Get current dates and timestamps\n",
    "- Add or subtract days\n",
    "- Find differences between dates\n",
    "- Convert strings into dates with custom formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa77b6",
   "metadata": {},
   "source": [
    "### üîé Step 1: Create a DataFrame with Date & Timestamp\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `current_date()` ‚Üí current system date (no time).\n",
    "\n",
    "- `current_timestamp()` ‚Üí current system date + time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e76a17eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+--------------------------+\n",
      "|id |today     |now                       |\n",
      "+---+----------+--------------------------+\n",
      "|0  |2025-09-12|2025-09-12 21:17:22.185137|\n",
      "|1  |2025-09-12|2025-09-12 21:17:22.185137|\n",
      "|2  |2025-09-12|2025-09-12 21:17:22.185137|\n",
      "|3  |2025-09-12|2025-09-12 21:17:22.185137|\n",
      "|4  |2025-09-12|2025-09-12 21:17:22.185137|\n",
      "+---+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(5) \\\n",
    "    .withColumn(\"today\", current_date()) \\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.printSchema()\n",
    "dateDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928b716",
   "metadata": {},
   "source": [
    "### üîé Step 2: Add & Subtract Days\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `date_sub(..., 5)` ‚Üí subtracts 5 days.\n",
    "\n",
    "- `date_add(..., 5)` ‚Üí adds 5 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a29fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|5_days_ago|5_days_later|\n",
      "+----------+------------+\n",
      "|2025-09-07|  2025-09-17|\n",
      "+----------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, col\n",
    "\n",
    "dateDF.select(\n",
    "    date_sub(col(\"today\"), 5).alias(\"5_days_ago\"),\n",
    "    date_add(col(\"today\"), 5).alias(\"5_days_later\")\n",
    ").show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28719ae4",
   "metadata": {},
   "source": [
    "### üîé Step 3: Date Differences\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `datediff` ‚Üí gives number of days between two dates.\n",
    "\n",
    "- `months_between` ‚Üí gives fractional months difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df75007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|days_diff|\n",
      "+---------+\n",
      "|       -7|\n",
      "+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+\n",
      "|months_diff|\n",
      "+-----------+\n",
      "|16.67741935|\n",
      "+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)) \\\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\")).alias(\"days_diff\")) \\\n",
    "    .show(1)\n",
    "\n",
    "dateDF.select(\n",
    "    months_between(lit(\"2017-05-22\"), lit(\"2016-01-01\")).alias(\"months_diff\")\n",
    ").show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e51572",
   "metadata": {},
   "source": [
    "### üîé Step 4: Converting Strings to Dates\n",
    "\n",
    "üëâ **Note**: If Spark cannot parse the string, it will return `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ba162a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|parsed_date|\n",
      "+-----------+\n",
      "| 2017-01-01|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "# Default format: yyyy-MM-dd\n",
    "spark.range(1).withColumn(\"date_str\", lit(\"2017-01-01\")) \\\n",
    "    .select(to_date(col(\"date_str\")).alias(\"parsed_date\")) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674759a7",
   "metadata": {},
   "source": [
    "### üîé Step 5: Handling Custom Formats\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- Format `\"yyyy-dd-MM\"` expects `year-day-month`.\n",
    "- `\"2017-12-11\"` parses fine (11th Dec 2017).\n",
    "- `\"2017-20-12\"` fails ‚Üí returns `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3de1d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|valid_date|invalid_date|\n",
      "+----------+------------+\n",
      "|2017-11-12|        NULL|\n",
      "+----------+------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|to_timestamp(valid_date, yyyy-dd-MM)|\n",
      "+------------------------------------+\n",
      "|                 2017-11-12 00:00:00|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"valid_date\"),\n",
    "    to_date(lit(\"2017-20-13\"), dateFormat).alias(\"invalid_date\")\n",
    ")\n",
    "\n",
    "cleanDateDF.show()\n",
    "\n",
    "# You can also convert to timestamp:\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "cleanDateDF.select(to_timestamp(col(\"valid_date\"), dateFormat)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104df145",
   "metadata": {},
   "source": [
    "### üîé Step 6: Why is this important?\n",
    "\n",
    "- Dates are **central in business data** (sales dates, order deadlines, retention analysis).\n",
    "- Spark gives flexibility to **parse different formats**, calculate differences, and adjust for time.\n",
    "- Knowing how Spark silently returns `null` on bad formats helps avoid hidden data issues.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Spark makes it easy to work with dates and timestamps ‚Äî you can get today‚Äôs date, add/subtract days, calculate differences, and convert string dates into real date objects (with custom formats when needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6f952",
   "metadata": {},
   "source": [
    "# üìí Section 7: Working with Nulls\n",
    "\n",
    "In real-world datasets, missing values are very common.\n",
    "\n",
    "Spark represents missing data as **`null`** (not `NaN` or empty string).\n",
    "\n",
    "Spark provides the **`.na` subpackage** on DataFrames for handling nulls:\n",
    "\n",
    "- `drop()` ‚Üí remove rows with nulls\n",
    "- `fill()` ‚Üí replace nulls with values\n",
    "- `replace()` ‚Üí replace specific values (not just nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef785a5",
   "metadata": {},
   "source": [
    "### üîé Step 1: Dropping Nulls\n",
    "\n",
    "Remove rows that contain null values.\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `\"any\"` ‚Üí drop row if **any column** is null.\n",
    "- `\"all\"` ‚Üí drop row only if **all columns** are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33eba5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with any null values\n",
    "df.na.drop().show(5)\n",
    "\n",
    "# Equivalent (default is \"any\")\n",
    "df.na.drop(\"any\").show(5)\n",
    "\n",
    "# Drop only rows where all values are null\n",
    "df.na.drop(\"all\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13ab33",
   "metadata": {},
   "source": [
    "### üîé Step 2: Filling Nulls\n",
    "\n",
    "Replace nulls with specified values.\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- Strings can be filled into text columns.\n",
    "- Numbers can be filled into numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cca3eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill all nulls with a single string\n",
    "df.na.fill(\"All Null values become this string\").show(5)\n",
    "\n",
    "# Fill with different values for specific columns\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\": \"No Value\"}\n",
    "df.na.fill(fill_cols_vals).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b672541f",
   "metadata": {},
   "source": [
    "### üîé Step 3: Replacing Values\n",
    "\n",
    "You can replace not just nulls but any value (like `?` or `NA`).\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- All occurrences of `\"?\"` and `\"NA\"` are replaced with `\"Unknown\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85d92cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.replace([\"?\", \"NA\"], \"Unknown\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea24a7",
   "metadata": {},
   "source": [
    "### üîé Step 4: Why is this important?\n",
    "\n",
    "- **Nulls affect calculations** (e.g., averages, joins, filters).\n",
    "- Cleaning nulls ensures consistent results.\n",
    "- Using `.na.drop()` or `.na.fill()` makes handling nulls **scalable and optimized** in Spark.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Null handling in Spark is done with `.na`. You can **drop rows, fill them with defaults, or replace specific values**. Always handle nulls before analysis to avoid incorrect results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e0da9",
   "metadata": {},
   "source": [
    "# üìí Section 8: Working with Complex Types\n",
    "\n",
    "Spark supports **complex data types** that let you organize nested or multi-valued data:\n",
    "\n",
    "- **Structs** ‚Üí like a nested row (columns inside a column)\n",
    "- **Arrays** ‚Üí ordered lists of values\n",
    "- (Later we‚Äôll also touch JSON, which combines these ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91864a3b",
   "metadata": {},
   "source": [
    "## üîπ 8.1 Structs\n",
    "\n",
    "A **struct** is like a DataFrame inside a column. You can group multiple fields into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aaf1e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|complex                                     |\n",
      "+--------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|\n",
      "|{WHITE METAL LANTERN, 536365}               |\n",
      "+--------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "# Create a struct column\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4a2eb",
   "metadata": {},
   "source": [
    "**Accessing struct fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "649eda0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+\n",
      "|complex.InvoiceNo|\n",
      "+-----------------+\n",
      "|           536365|\n",
      "|           536365|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dot syntax\n",
    "complexDF.select(\"complex.Description\").show(2)\n",
    "\n",
    "# Using getField\n",
    "complexDF.select(col(\"complex\").getField(\"InvoiceNo\")).show(2)\n",
    "\n",
    "# Expanding all fields\n",
    "complexDF.select(\"complex.*\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77ea18",
   "metadata": {},
   "source": [
    "## üîπ 8.2 Arrays\n",
    "\n",
    "An **array** is a list of values stored in a column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9f265",
   "metadata": {},
   "source": [
    "**Splitting strings into arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fd0e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|array_col                               |\n",
      "+----------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|[WHITE, METAL, LANTERN]                 |\n",
      "+----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).show(2, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f8ecf",
   "metadata": {},
   "source": [
    "**Accessing array elements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0e2ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")) \\\n",
    "  .selectExpr(\"array_col[0]\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7eff5",
   "metadata": {},
   "source": [
    "**Array length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4e7a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|array_size|\n",
      "+----------+\n",
      "|         5|\n",
      "|         3|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \")).alias(\"array_size\")).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a8bfe",
   "metadata": {},
   "source": [
    "**Checking if an array contains a value**\n",
    "\n",
    "üëâ Returns `True/False` depending on whether \"WHITE\" exists in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61d6206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4863ce8",
   "metadata": {},
   "source": [
    "## üîπ 8.3 Exploding Arrays\n",
    "\n",
    "`explode()` turns **one row with an array** into **multiple rows (one per element)**.\n",
    "\n",
    "üëâ Example: `\"WHITE METAL LANTERN\"` ‚Üí becomes 3 rows: `\"WHITE\"`, `\"METAL\"`, `\"LANTERN\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "760c0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \")) \\\n",
    "  .withColumn(\"exploded\", explode(col(\"splitted\"))) \\\n",
    "  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89abcdb",
   "metadata": {},
   "source": [
    "### üîé Why is this important?\n",
    "\n",
    "- **Structs** ‚Üí Useful for grouping related columns together (like address: {city, state, zip}).\n",
    "- **Arrays** ‚Üí Useful for handling multi-valued fields (like tags, categories, words).\n",
    "- **Explode** ‚Üí Lets you normalize arrays into rows for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Structs are ‚Äúcolumns within a column‚Äù, arrays are ‚Äúlists inside a column‚Äù, and explode helps turn arrays into rows so you can analyze them easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7115dbf",
   "metadata": {},
   "source": [
    "# üìí Section 9: Working with JSON\n",
    "\n",
    "JSON is a very common data format in real-world datasets (APIs, logs, configs).\n",
    "\n",
    "Spark has **built-in functions** to parse JSON strings and extract values from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490e1fb",
   "metadata": {},
   "source": [
    "### üîé Step 1: Create a JSON Column\n",
    "\n",
    "Let‚Äôs make a small DataFrame containing a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f932818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\n",
    "    \"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\"\n",
    ")\n",
    "\n",
    "jsonDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac253752",
   "metadata": {},
   "source": [
    "### üîé Step 2: Extract Nested JSON Values with `get_json_object`\n",
    "\n",
    "`get_json_object` lets you query JSON using **JSONPath syntax** (`$.path.to.value`).\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `$.myJSONKey.myJSONValue[1]` ‚Üí gets the **2nd element** (index 1) of the array `[1,2,3]`.\n",
    "- Returns: `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4b84b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|second_element|\n",
      "+--------------+\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, col\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"second_element\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a1512",
   "metadata": {},
   "source": [
    "### üîé Step 3: Extract Top-Level Fields with `json_tuple`\n",
    "\n",
    "If JSON has a **single-level structure**, `json_tuple` is more convenient.\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `json_tuple` extracts the field `\"myJSONKey\"` from the JSON string.\n",
    "- Output is still a JSON object string: `{\"myJSONValue\":[1,2,3]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f76f0a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|parsed_key             |\n",
      "+-----------------------+\n",
      "|{\"myJSONValue\":[1,2,3]}|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\").alias(\"parsed_key\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51e395",
   "metadata": {},
   "source": [
    "### üîé Step 4: Why is this important?\n",
    "\n",
    "- JSON is everywhere (web APIs, IoT logs, nested event data).\n",
    "- With `get_json_object`, you can extract **deeply nested values** without converting the whole JSON into columns.\n",
    "- With `json_tuple`, you can quickly grab **top-level fields**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Spark lets you query JSON strings inside columns. Use `get_json_object` for nested fields, and `json_tuple` for flat JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1108b8f",
   "metadata": {},
   "source": [
    "# üìí Section 9 (Bonus): Reading JSON Files into DataFrames\n",
    "\n",
    "Instead of working with JSON strings inside a column, Spark can **directly read JSON files** into a DataFrame.\n",
    "\n",
    "This is very common in pipelines where logs or data dumps are stored in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f0e7c",
   "metadata": {},
   "source": [
    "### üîé Step 1: Reading a JSON File\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `format(\"json\")` tells Spark the file is JSON.\n",
    "- Spark automatically **infers the schema** by reading the JSON keys.\n",
    "- Each JSON object = one row in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac7c73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load JSON file\n",
    "json_path = get_project_path('data', 'darshil-data', 'flight-data', 'json', '2015-summary.json')\n",
    "df_json = spark.read.format(\"json\").load(json_path)\n",
    "\n",
    "# Inspect schema and data\n",
    "df_json.printSchema()\n",
    "df_json.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a457c3b",
   "metadata": {},
   "source": [
    "### üîé Step 2: Querying the JSON DataFrame\n",
    "\n",
    "Now you can work with it just like any other DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37926fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|            Grenada|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|   15|\n",
      "|    United States|    1|\n",
      "|    United States|  344|\n",
      "|            Egypt|   15|\n",
      "|    United States|   62|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+----------+\n",
      "|DEST_COUNTRY_NAME|sum(count)|\n",
      "+-----------------+----------+\n",
      "|         Anguilla|        41|\n",
      "|           Russia|       176|\n",
      "|         Paraguay|        60|\n",
      "|          Senegal|        40|\n",
      "|           Sweden|       118|\n",
      "+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter\n",
    "df_json.filter(col(\"count\") > 10).show(5)\n",
    "\n",
    "# Select specific columns\n",
    "df_json.select(\"DEST_COUNTRY_NAME\", \"count\").show(5)\n",
    "\n",
    "# Aggregate\n",
    "df_json.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e44c8e",
   "metadata": {},
   "source": [
    "### üîé Step 4: Why is this important?\n",
    "\n",
    "- JSON is a **native file format** Spark can parse directly (like CSV, Parquet).\n",
    "- No need to manually parse strings ‚Üí Spark maps JSON keys to DataFrame columns.\n",
    "- Schema inference makes it quick, but you can also define schemas manually for strict control.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "You can either **parse JSON strings inside a DataFrame** using `get_json_object`/`json_tuple`, or **load JSON files directly into a DataFrame** with `spark.read.json`. Both are essential depending on your data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138e245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
