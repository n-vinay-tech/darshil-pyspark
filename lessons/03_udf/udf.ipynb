{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31d84d8",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ba70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43935ec2",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c123311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a26f18",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc217d",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e01c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bd6a6",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800f5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22a5d0",
   "metadata": {},
   "source": [
    "Import pyspark package and create a spark sesstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d685dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UDF Example\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49959f1",
   "metadata": {},
   "source": [
    "# üìí User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82871064",
   "metadata": {},
   "source": [
    "### üîé Step 1: What are UDFs?\n",
    "\n",
    "- UDFs = **User Defined Functions**.\n",
    "- They let you write **custom functions** in Python/Scala/Java and run them on Spark DataFrames.\n",
    "- Spark distributes your function to all worker nodes, applies it row by row, and collects results.\n",
    "\n",
    "‚ö†Ô∏è With Python UDFs, Spark has to:\n",
    "\n",
    "1. Send data from JVM ‚Üí Python process\n",
    "2. Run function row by row\n",
    "3. Send results back to JVM\n",
    "\n",
    "This makes UDFs **slower** than built-in Spark functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc395301",
   "metadata": {},
   "source": [
    "### üîé Step 2: Create a Simple Python Function\n",
    "\n",
    "üëâ Always test your function locally before registering it with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a396fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with a column 'num'\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "# Define a Python function (cube a number)\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "# Test locally\n",
    "print(power3(2.0))  # Output: 8.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41848523",
   "metadata": {},
   "source": [
    "### üîé Step 3: Register as a UDF and Use in DataFrame API\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `udf(power3)` wraps your Python function for Spark.\n",
    "- It runs **row by row** on workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc0026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|num_cubed|\n",
      "+---------+\n",
      "|        0|\n",
      "|        1|\n",
      "|        8|\n",
      "|       27|\n",
      "|       64|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# Convert Python function into a Spark UDF\n",
    "power3udf = udf(power3)\n",
    "\n",
    "# Apply it on the DataFrame\n",
    "udfExampleDF.select(power3udf(col(\"num\")).alias(\"num_cubed\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbf71e",
   "metadata": {},
   "source": [
    "### üîé Step 4: Using UDFs in SQL Expressions\n",
    "\n",
    "To call UDFs inside `selectExpr` or `spark.sql`, you must **register** them with Spark SQL.\n",
    "üëâ Now, `power3` behaves like a built-in SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807ade6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register UDF with Spark SQL\n",
    "spark.udf.register(\"power3\", power3)\n",
    "\n",
    "# Use in SQL-style expressions\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd91a1",
   "metadata": {},
   "source": [
    "### üîé Step 5: Why is this important?\n",
    "\n",
    "- UDFs let you add **custom logic** Spark doesn‚Äôt provide out of the box.\n",
    "- Useful for things like:\n",
    "    - Custom math formulas\n",
    "    - Text cleaning with regex\n",
    "    - Integrating external Python libraries\n",
    "- But:\n",
    "    - ‚ö° **Built-in Spark functions are faster** ‚Üí prefer them first.\n",
    "    - ‚úÖ Use UDFs only when necessary.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "UDFs let you define your own logic in Python and run it in Spark, but they are slower than built-in functions. Use them when Spark doesn‚Äôt already have a function for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58f74b",
   "metadata": {},
   "source": [
    "# üìí Typed UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206fcd24",
   "metadata": {},
   "source": [
    "### üîé Step 1: Why do we need typed UDFs?\n",
    "\n",
    "- By default, Spark assumes UDFs return **string type**, unless you explicitly specify.\n",
    "- This can cause wrong schemas or unexpected results.\n",
    "- To make UDFs safer and faster, always provide a **return type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a468b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_cubed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply it on the DataFrame\n",
    "udfExampleDF.select(power3udf(col(\"num\")).alias(\"num_cubed\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867c1cf",
   "metadata": {},
   "source": [
    "### üîé Step 2: Defining a Typed UDF\n",
    "\n",
    "We can pass the return type using `udf(function, returnType)`.\n",
    "\n",
    "üëâ Explanation:\n",
    "\n",
    "- `DoubleType()` tells Spark the UDF will return floating-point numbers.\n",
    "- Schema is now **accurate** and Spark can optimize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd5aca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|num|num_cubed|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "|  4|       64|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "# Define Python function\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "# Register as typed UDF (returns DoubleType)\n",
    "power3udf = udf(power3, LongType())\n",
    "\n",
    "# Apply UDF\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "udfExampleDF.select(col(\"num\"), power3udf(col(\"num\")).alias(\"num_cubed\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614b853",
   "metadata": {},
   "source": [
    "### üîé Step 3: Using Typed UDFs in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e28e9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|num|num_cubed|\n",
      "+---+---------+\n",
      "|  0|        0|\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "|  4|       64|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register with return type\n",
    "spark.udf.register(\"power3\", power3, LongType())\n",
    "\n",
    "# Use in SQL-style expression\n",
    "udfExampleDF.selectExpr(\"num\", \"power3(num) as num_cubed\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cbbe7",
   "metadata": {},
   "source": [
    "### üîé Step 4: Why is this important?\n",
    "\n",
    "- Avoids schema inference mistakes.\n",
    "- Makes your UDFs **safer** (you won‚Äôt accidentally return wrong types).\n",
    "- Allows Spark to apply better **optimizations**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In simple words**:\n",
    "\n",
    "Always specify a return type when creating UDFs. It avoids errors and helps Spark run more efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
