{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce55757e",
   "metadata": {},
   "source": [
    "# Set ENV Variable to Project Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23e2eee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8d441",
   "metadata": {},
   "source": [
    "Insert project root folder in environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52049ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\ds_analytics_projects\\darshil_course\\apache-pyspark\\darshil-pyspark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root(start_path=None, markers=(\".git\", \"pyproject.toml\", \"requirements.txt\")):\n",
    "    \"\"\"\n",
    "    Walks up from start_path until it finds one of the marker files/folders.\n",
    "    Returns the path of the project root.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "\n",
    "    current_path = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        # check if any marker exists in current path\n",
    "        if any(os.path.exists(os.path.join(current_path, marker)) for marker in markers):\n",
    "            return current_path\n",
    "\n",
    "        new_path = os.path.dirname(current_path)  # parent folder\n",
    "        if new_path == current_path:  # reached root of filesystem\n",
    "            raise FileNotFoundError(f\"None of the markers {markers} found above {start_path}\")\n",
    "        current_path = new_path\n",
    "\n",
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb1519",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90fbf5",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08a5f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c3394",
   "metadata": {},
   "source": [
    "Relative import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f564d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_utils import get_project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ad52b",
   "metadata": {},
   "source": [
    "# Apache Spark Data Sources\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers Spark's unified data source API, exploring how to read and write data across different formats and systems with consistent syntax and powerful optimization features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6525e",
   "metadata": {},
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72319db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.6\n",
      "Spark Context: DataSourcesDemo\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataSourcesDemo\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Context: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b509f",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 1: What are Spark Data Sources?\n",
    "\n",
    "Spark provides a **unified API** for reading and writing data across multiple formats and systems:\n",
    "\n",
    "### Six Core Built-in Sources:\n",
    "1. **CSV** - Comma-separated values\n",
    "2. **JSON** - JavaScript Object Notation\n",
    "3. **Parquet** - Columnar storage (default format)\n",
    "4. **ORC** - Optimized Row Columnar\n",
    "5. **JDBC/ODBC** - Database connections\n",
    "6. **Plain-text files** - Raw text data\n",
    "\n",
    "### Community Connectors:\n",
    "- Avro, Delta Lake, MongoDB, Cassandra, Elasticsearch, and hundreds more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520eedf4",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 2: The General API Structure\n",
    "\n",
    "### Read API Pattern\n",
    "```python\n",
    "DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n",
    "```\n",
    "\n",
    "### Write API Pattern  \n",
    "```python\n",
    "DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).save()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b36b729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create some sample data to work with throughout this notebook\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create sample flight data\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"United States\", \"Romania\", 15),\n",
    "    (\"United States\", \"Croatia\", 1),\n",
    "    (\"United States\", \"Ireland\", 344),\n",
    "    (\"Egypt\", \"United States\", 15),\n",
    "    (\"United States\", \"India\", 62),\n",
    "    (\"United States\", \"Singapore\", 1),\n",
    "    (\"United States\", \"Grenada\", 62),\n",
    "    (\"Costa Rica\", \"United States\", 588),\n",
    "    (\"Senegal\", \"United States\", 40),\n",
    "    (\"Moldova\", \"United States\", 1)\n",
    "]\n",
    "\n",
    "sample_df = spark.createDataFrame(data, schema)\n",
    "sample_df.show()\n",
    "# print(f\"Sample data created with {sample_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74384a55",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 3: Read Modes (for handling bad data)\n",
    "\n",
    "Spark provides three modes to handle malformed records:\n",
    "\n",
    "- **`permissive`** (default): Sets malformed fields to `null`, stores full record in `_corrupt_record`\n",
    "- **`dropMalformed`**: Drops bad rows entirely\n",
    "- **`failFast`**: Stops immediately when encountering malformed record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f2176c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERMISSIVE Mode (default) ===\n",
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1|  Alice|  30|\n",
      "|  2|    Bob|NULL|\n",
      "|  3|Charlie|  25|\n",
      "|  4|   Dora|NULL|\n",
      "|  5|    Eve|  45|\n",
      "+---+-------+----+\n",
      "\n",
      "Rows in permissive mode: 5\n",
      "\n",
      "=== DROPMALFORMED Mode ===\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  3|Charlie| 25|\n",
      "+---+-------+---+\n",
      "\n",
      "Rows in dropMalformed mode: 5\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate read modes with intentionally malformed CSV data\n",
    "malformed_csv_data = \"\"\"id,name,age\n",
    "1,Alice,30\n",
    "2,Bob,not_a_number\n",
    "3,Charlie,25\n",
    "4,Dora\n",
    "5,Eve,45,extra_column\n",
    "\"\"\"\n",
    "\n",
    "# Write malformed data to temp location\n",
    "malformed_csv_path = get_project_path('lessons', '05_io', 'temp', 'malformed_data.csv')\n",
    "with open(malformed_csv_path, 'w') as f:\n",
    "    f.write(malformed_csv_data)\n",
    "\n",
    "# Create manualSchema\n",
    "manual_schema = StructType(\n",
    "    [\n",
    "        StructField(name=\"id\", dataType=LongType(), nullable=True),\n",
    "        StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "        StructField(name=\"age\", dataType=LongType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== PERMISSIVE Mode (default) ===\")\n",
    "permissive_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(malformed_csv_path, schema=manual_schema)\n",
    "\n",
    "permissive_df.show()\n",
    "print(f\"Rows in permissive mode: {permissive_df.count()}\")\n",
    "\n",
    "print(\"\\n=== DROPMALFORMED Mode ===\")\n",
    "drop_malformed_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(malformed_csv_path, schema=manual_schema)\n",
    "\n",
    "drop_malformed_df.show()\n",
    "print(f\"Rows in dropMalformed mode: {drop_malformed_df.count()}\")\n",
    "\n",
    "# print(\"\\n=== FAILFAST Mode ===\")\n",
    "# drop_malformed_df = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"mode\", \"FAILFAST\") \\\n",
    "#     .load(malformed_csv_path, schema=manual_schema)\n",
    "\n",
    "# drop_malformed_df.show()\n",
    "# print(f\"Rows in dropMalformed mode: {drop_malformed_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3fbad",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Step 4: Save Modes (when writing data)\n",
    "\n",
    "- **`append`** â†’ Add new data to existing\n",
    "- **`overwrite`** â†’ Replace existing data completely  \n",
    "- **`errorIfExists`** (default) â†’ Fail if data already exists\n",
    "- **`ignore`** â†’ Skip writing if data already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8bef278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First write (should succeed) ===\n",
      "Initial data written successfully\n",
      "\n",
      "=== Reading back the data ===\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "Initial count: 3\n",
      "\n",
      "=== Append mode ===\n",
      "After append count: 5\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "\n",
      "=== Overwrite mode ===\n",
      "After overwrite count: 1\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate save modes\n",
    "test_path = str(Path(get_project_path('lessons', '05_io', 'temp', 'save_modes')))\n",
    "\n",
    "print(\"=== First write (should succeed) ===\")\n",
    "sample_df.limit(3).write.format(\"parquet\").mode(\"overwrite\").save(test_path)\n",
    "print(\"Initial data written successfully\")\n",
    "\n",
    "print(\"\\n=== Reading back the data ===\")\n",
    "initial_read = spark.read.format(\"parquet\").load(test_path)\n",
    "initial_read.show()\n",
    "print(f\"Initial count: {initial_read.count()}\")\n",
    "\n",
    "print(\"\\n=== Append mode ===\")\n",
    "sample_df.limit(2).write.format(\"parquet\").mode(\"append\").save(test_path)\n",
    "appended_read = spark.read.format(\"parquet\").load(test_path)\n",
    "print(f\"After append count: {appended_read.count()}\")\n",
    "appended_read.show()\n",
    "\n",
    "print(\"\\n=== Overwrite mode ===\")\n",
    "sample_df.limit(1).write.format(\"parquet\").mode(\"overwrite\").save(test_path)\n",
    "overwritten_read = spark.read.format(\"parquet\").load(test_path)\n",
    "print(f\"After overwrite count: {overwritten_read.count()}\")\n",
    "overwritten_read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a252a",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.1 CSV Files\n",
    "\n",
    "CSV is the most common format for data exchange, though not the most efficient for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3f0c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading CSV with various options ===\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "\n",
      "=== Writing CSV with custom separator (TSV) ===\n",
      "TSV file created successfully\n"
     ]
    }
   ],
   "source": [
    "# === CSV Reading Examples ===\n",
    "csv_path = str(Path(get_project_path('data', 'darshil-data', 'flight-data', 'csv', '2010-summary.csv')))\n",
    "out_dir_tsv = str(Path(get_project_path('lessons', '05_io', 'temp', 'tsv')))\n",
    "\n",
    "print(\"\\n=== Reading CSV with various options ===\")\n",
    "csv_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "csv_df.show(5)\n",
    "csv_df.printSchema()\n",
    "\n",
    "print(\"\\n=== Writing CSV with custom separator (TSV) ===\")\n",
    "csv_df.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(out_dir_tsv)\n",
    "\n",
    "print(\"TSV file created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ed19e",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.2 JSON Files\n",
    "\n",
    "JSON is excellent for semi-structured data with nested fields and varying schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9892e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading JSON with options ===\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "\n",
      "=== Writing JSON ===\n",
      "JSON file created successfully\n"
     ]
    }
   ],
   "source": [
    "# === JSON Reading and Writing ===\n",
    "json_path = str(Path(get_project_path('data', 'darshil-data', 'flight-data', 'json', '2010-summary.json')))\n",
    "out_dir_json = str(Path(get_project_path('lessons', '05_io', 'temp', 'json')))\n",
    "\n",
    "print(\"\\n=== Reading JSON with options ===\")\n",
    "json_df = spark.read.format(\"json\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(json_path)\n",
    "\n",
    "json_df.show(5)\n",
    "json_df.printSchema()\n",
    "\n",
    "print(\"\\n=== Writing JSON ===\")\n",
    "json_df.write.format(\"json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(out_dir_json)\n",
    "\n",
    "print(\"JSON file created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f48621",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.3 Parquet Files\n",
    "\n",
    "**Parquet is Spark's default format** - columnar storage optimized for analytics workloads.\n",
    "\n",
    "### Benefits:\n",
    "- **Columnar storage** â†’ efficient compression and querying\n",
    "- **Schema evolution** â†’ add/remove columns over time\n",
    "- **Predicate pushdown** â†’ filter data at file level\n",
    "- **Complex types** â†’ arrays, maps, structs supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e95a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading from Parquet ===\n",
      "\n",
      "=== Add Cols to Parquet file ===\n",
      "\n",
      "=== Parquet file info ===\n",
      "Row count: 255\n",
      "Schema:\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- ORIGIN_DEST_ARRAY: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ROW_STRUCT: struct (nullable = false)\n",
      " |    |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |    |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |    |-- count: long (nullable = true)\n",
      "\n",
      "\n",
      "=== Writing Parquet ===\n",
      "JSON file created successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col, array\n",
    "\n",
    "# === Parquet Operations ===\n",
    "parquet_path = str(Path(get_project_path('data', 'darshil-data', 'flight-data', 'parquet', '2010-summary.parquet')))\n",
    "out_dir_parquet = str(Path(get_project_path('lessons', '05_io', 'temp', 'parquet')))\n",
    "\n",
    "print(\"\\n=== Reading from Parquet ===\")\n",
    "parquet_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(parquet_path)\n",
    "\n",
    "print(\"\\n=== Add Cols to Parquet file ===\")\n",
    "parquet_df = parquet_df.select(\"*\",\n",
    "                               array(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").alias(\"ORIGIN_DEST_ARRAY\"),\n",
    "                               struct(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\", \"count\").alias(\"ROW_STRUCT\")\n",
    "                            )\n",
    "\n",
    "print(\"\\n=== Parquet file info ===\")\n",
    "print(f\"Row count: {parquet_df.count()}\")\n",
    "print(\"Schema:\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "print(\"\\n=== Writing Parquet ===\")\n",
    "parquet_df.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(out_dir_parquet)\n",
    "\n",
    "print(\"JSON file created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e2162",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.4 ORC Files\n",
    "\n",
    "**ORC (Optimized Row Columnar)** is optimized for Hadoop ecosystems and great for large sequential reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7760e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading from ORC ===\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "=== Writing to ORC format ===\n",
      "\n",
      "ORC file successfully processed 255 rows\n"
     ]
    }
   ],
   "source": [
    "# === ORC Operations ===\n",
    "orc_path = str(Path(get_project_path('data', 'darshil-data', 'flight-data', 'orc', '2010-summary.orc')))\n",
    "out_dir_orc = str(Path(get_project_path('lessons', '05_io', 'temp', 'orc')))\n",
    "\n",
    "print(\"\\n=== Reading from ORC ===\")\n",
    "orc_df = spark.read.format(\"orc\").load(orc_path)\n",
    "orc_df.show(5)\n",
    "orc_df.printSchema()\n",
    "\n",
    "print(\"=== Writing to ORC format ===\")\n",
    "orc_df.write.format(\"orc\").mode(\"overwrite\").save(out_dir_orc)\n",
    "\n",
    "print(f\"\\nORC file successfully processed {orc_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513a6c6",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.5 Text Files\n",
    "\n",
    "Plain text files are useful for unstructured data processing and custom parsing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37fc169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading text file (CSV as raw text) ===\n",
      "+-------------------------------------------+\n",
      "|value                                      |\n",
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "|United States,Romania,1                    |\n",
      "|United States,Ireland,264                  |\n",
      "|United States,India,69                     |\n",
      "|Egypt,United States,24                     |\n",
      "+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Processing text data with custom parsing ===\n",
      "+--------------------+----------------+-----+\n",
      "|         destination|          origin|count|\n",
      "+--------------------+----------------+-----+\n",
      "|       United States|         Romania|    1|\n",
      "|       United States|         Ireland|  264|\n",
      "|       United States|           India|   69|\n",
      "|               Egypt|   United States|   24|\n",
      "|   Equatorial Guinea|   United States|    1|\n",
      "|       United States|       Singapore|   25|\n",
      "|       United States|         Grenada|   54|\n",
      "|          Costa Rica|   United States|  477|\n",
      "|             Senegal|   United States|   29|\n",
      "|       United States|Marshall Islands|   44|\n",
      "|              Guyana|   United States|   17|\n",
      "|       United States|    Sint Maarten|   53|\n",
      "|               Malta|   United States|    1|\n",
      "|             Bolivia|   United States|   46|\n",
      "|            Anguilla|   United States|   21|\n",
      "|Turks and Caicos ...|   United States|  136|\n",
      "|       United States|     Afghanistan|    2|\n",
      "|Saint Vincent and...|   United States|    1|\n",
      "|               Italy|   United States|  390|\n",
      "|       United States|          Russia|  156|\n",
      "+--------------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "=== Writing selected column as text ===\n",
      "Text file written successfully\n"
     ]
    }
   ],
   "source": [
    "# === Text File Operations ===\n",
    "out_dir_txt = str(Path(get_project_path('lessons', '05_io', 'temp', 'txt')))\n",
    "\n",
    "print(\"=== Reading text file (CSV as raw text) ===\")\n",
    "text_df = spark.read.text(csv_path)\n",
    "text_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n=== Processing text data with custom parsing ===\")\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Skip header and parse CSV manually\n",
    "parsed_text = text_df.filter(~col(\"value\").contains(\"DEST_COUNTRY_NAME\")) \\\n",
    "    .select(split(col(\"value\"), \",\").alias(\"rows\")) \\\n",
    "    .select(\n",
    "        col(\"rows\")[0].alias(\"destination\"),\n",
    "        col(\"rows\")[1].alias(\"origin\"), \n",
    "        col(\"rows\")[2].cast(\"int\").alias(\"count\")\n",
    "    )\n",
    "\n",
    "parsed_text.show()\n",
    "\n",
    "print(\"\\n=== Writing selected column as text ===\")\n",
    "parsed_text.select(\"destination\").write.mode(\"overwrite\").text(out_dir_txt)\n",
    "print(\"Text file written successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ff99f",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.6 JDBC Connections\n",
    "\n",
    "Spark can connect to relational databases for reading and writing data.\n",
    "\n",
    "**Note:** This example shows the syntax. In a real environment, you'd need:\n",
    "- JDBC driver JAR files\n",
    "- Running database instance\n",
    "- Proper network connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "488e9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JDBC Read Syntax ===\n",
      "\n",
      "# Reading from database\n",
      "jdbcDF = spark.read.format(\"jdbc\")     .option(\"url\", \"jdbc:mysql://localhost:3306/mydb\")     .option(\"dbtable\", \"employees\")     .option(\"user\", \"root\")     .option(\"password\", \"mypassword\")     .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")     .load()\n",
      "\n",
      "\n",
      "=== JDBC Write Syntax ===\n",
      "\n",
      "# Writing to database  \n",
      "df.write.format(\"jdbc\")     .option(\"url\", \"jdbc:mysql://localhost:3306/mydb\")     .option(\"dbtable\", \"employees_backup\")     .option(\"user\", \"root\")     .option(\"password\", \"mypassword\")     .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")     .mode(\"overwrite\")     .save()\n",
      "\n",
      "\n",
      "=== Advanced JDBC Options ===\n",
      "\n",
      "Key JDBC options:\n",
      "- numPartitions: Control parallelism\n",
      "- fetchsize: Rows fetched per round trip\n",
      "- batchsize: Rows inserted per batch\n",
      "- isolationLevel: Transaction isolation\n",
      "- sessionInitStatement: Setup SQL per session\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === JDBC Examples (syntax demonstration) ===\n",
    "print(\"=== JDBC Read Syntax ===\")\n",
    "jdbc_read_example = \"\"\"\n",
    "# Reading from database\n",
    "jdbcDF = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/mydb\") \\\n",
    "    .option(\"dbtable\", \"employees\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"mypassword\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\"\"\"\n",
    "print(jdbc_read_example)\n",
    "\n",
    "print(\"\\n=== JDBC Write Syntax ===\")\n",
    "jdbc_write_example = \"\"\"\n",
    "# Writing to database  \n",
    "df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/mydb\") \\\n",
    "    .option(\"dbtable\", \"employees_backup\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"mypassword\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\"\"\"\n",
    "print(jdbc_write_example)\n",
    "\n",
    "print(\"\\n=== Advanced JDBC Options ===\")\n",
    "advanced_options = \"\"\"\n",
    "Key JDBC options:\n",
    "- numPartitions: Control parallelism\n",
    "- fetchsize: Rows fetched per round trip\n",
    "- batchsize: Rows inserted per batch\n",
    "- isolationLevel: Transaction isolation\n",
    "- sessionInitStatement: Setup SQL per session\n",
    "\"\"\"\n",
    "print(advanced_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0548e2",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.7 Advanced I/O Concepts\n",
    "\n",
    "### 1. Writing in Parallel\n",
    "Control parallelism with partitioning - one file per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a99d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original partitions: 12 ===\n",
      "\n",
      "=== Writing with 5 partitions ===\n",
      "Number of CSV files created: 4\n",
      "Files: ['part-00000-0616e57a-5d97-4a8a-b78e-bd46ce7578c1-c000.csv', 'part-00001-0616e57a-5d97-4a8a-b78e-bd46ce7578c1-c000.csv', 'part-00003-0616e57a-5d97-4a8a-b78e-bd46ce7578c1-c000.csv']...\n",
      "\n",
      "=== Writing with 1 partition (single file) ===\n",
      "Number of CSV files created: 1\n"
     ]
    }
   ],
   "source": [
    "# === Parallel Writing ===\n",
    "print(f\"=== Original partitions: {sample_df.rdd.getNumPartitions()} ===\")\n",
    "\n",
    "print(\"\\n=== Writing with 5 partitions ===\")\n",
    "parallel_path = \"/tmp/parallel_write\"\n",
    "sample_df.repartition(5).write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(parallel_path)\n",
    "\n",
    "# Check number of files created\n",
    "import os\n",
    "files = [f for f in os.listdir(parallel_path) if f.endswith('.csv')]\n",
    "print(f\"Number of CSV files created: {len(files)}\")\n",
    "print(f\"Files: {files[:3]}...\")  # Show first 3 files\n",
    "\n",
    "print(\"\\n=== Writing with 1 partition (single file) ===\")\n",
    "single_file_path = \"/tmp/single_file\"\n",
    "sample_df.coalesce(1).write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(single_file_path)\n",
    "\n",
    "single_files = [f for f in os.listdir(single_file_path) if f.endswith('.csv')]\n",
    "print(f\"Number of CSV files created: {len(single_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba940e3",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 15.7 Advanced I/O Concepts\n",
    "\n",
    "This section covers advanced techniques for optimizing data I/O performance in Spark through parallel writing, partitioning, and bucketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97e77c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created:\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Egypt|            Romania|   25|\n",
      "|            India|      United States|  125|\n",
      "|          Germany|      United States|   89|\n",
      "|           France|      United States|  156|\n",
      "|            Japan|      United States|   78|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "Total rows: 15\n",
      "Current partitions: 12\n"
     ]
    }
   ],
   "source": [
    "# Setup - Create sample data for demonstrations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "# Initialize Spark if not already done\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedIOConcepts\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample flight data (csvFile equivalent)\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"United States\", \"Romania\", 15),\n",
    "    (\"United States\", \"Croatia\", 1),\n",
    "    (\"United States\", \"Ireland\", 344),\n",
    "    (\"Egypt\", \"United States\", 15),\n",
    "    (\"United States\", \"India\", 62),\n",
    "    (\"United States\", \"Singapore\", 1),\n",
    "    (\"United States\", \"Grenada\", 62),\n",
    "    (\"Costa Rica\", \"United States\", 588),\n",
    "    (\"Senegal\", \"United States\", 40),\n",
    "    (\"Moldova\", \"United States\", 1),\n",
    "    (\"Egypt\", \"Romania\", 25),\n",
    "    (\"India\", \"United States\", 125),\n",
    "    (\"Germany\", \"United States\", 89),\n",
    "    (\"France\", \"United States\", 156),\n",
    "    (\"Japan\", \"United States\", 78)\n",
    "]\n",
    "\n",
    "csvFile = spark.createDataFrame(data, schema)\n",
    "print(\"Sample data created:\")\n",
    "csvFile.show()\n",
    "print(f\"Total rows: {csvFile.count()}\")\n",
    "print(f\"Current partitions: {csvFile.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a717e",
   "metadata": {},
   "source": [
    "### **1. Writing in Parallel**\n",
    "\n",
    "**Key Concept**: One file per partition.\n",
    "\n",
    "Controlling the number of partitions determines how many files are written in parallel, which affects both write performance and file organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ad2e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Writing with 5 partitions (5 files) ===\n"
     ]
    }
   ],
   "source": [
    "# === Writing in Parallel Demo ===\n",
    "print(\"=== 1. Writing with 5 partitions (5 files) ===\")\n",
    "out_dir_csv = str(Path(get_project_path('lessons', '05_io', 'temp', 'csv')))\n",
    "\n",
    "# Repartition to 5 partitions and write\n",
    "csv_df.repartition(5).write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(out_dir_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c56fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files created: 5\n",
      "Files: ['part-00000-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv', 'part-00001-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv', 'part-00002-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv', 'part-00003-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv', 'part-00004-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv']\n"
     ]
    }
   ],
   "source": [
    "# Check the files created\n",
    "csv_files = [f for f in os.listdir(out_dir_csv) if f.endswith('.csv')]\n",
    "print(f\"Number of CSV files created: {len(csv_files)}\")\n",
    "print(f\"Files: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90f757ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  part-00000-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv: 1538 bytes\n",
      "  part-00001-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv: 1566 bytes\n",
      "  part-00002-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv: 1480 bytes\n",
      "  part-00003-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv: 1491 bytes\n",
      "  part-00004-c606de9b-8c4b-4bdd-9ea9-f2b4feab6a34-c000.csv: 1482 bytes\n"
     ]
    }
   ],
   "source": [
    "# Show file sizes\n",
    "for file in csv_files:\n",
    "    file_path = Path(out_dir_csv)/f\"{file}\"\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  {file}: {size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87b087",
   "metadata": {},
   "source": [
    "### **2. Partitioning (Directory-based Organization)**\n",
    "\n",
    "**Key Concept**: Create directory structure based on column values for efficient querying.\n",
    "\n",
    "**Benefits**:\n",
    "- **Partition pruning**: Only read relevant directories\n",
    "- **Faster filtering**: Skip entire partitions during queries\n",
    "- **Organized storage**: Logical data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62cc0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2. Partitioning by DEST_COUNTRY_NAME ===\n",
      "\n",
      "ðŸ‘‰ **Directory structure created:**\n",
      "â”œâ”€â”€ DEST_COUNTRY_NAME=Costa%20Rica/\n",
      "â”‚   â””â”€â”€ part-00000-d40f60ce-5339-488b-b1ea-bf07b16dd907.c000.snappy.parquet\n",
      "â”œâ”€â”€ DEST_COUNTRY_NAME=Egypt/\n",
      "â”‚   â””â”€â”€ part-00000-d40f60ce-5339-488b-b1ea-bf07b16dd907.c000.snappy.parquet\n",
      "â”œâ”€â”€ DEST_COUNTRY_NAME=Equatorial%20Guinea/\n",
      "â”‚   â””â”€â”€ part-00000-d40f60ce-5339-488b-b1ea-bf07b16dd907.c000.snappy.parquet\n",
      "â”œâ”€â”€ DEST_COUNTRY_NAME=Senegal/\n",
      "â”‚   â””â”€â”€ part-00000-d40f60ce-5339-488b-b1ea-bf07b16dd907.c000.snappy.parquet\n",
      "â”œâ”€â”€ DEST_COUNTRY_NAME=United%20States/\n",
      "â”‚   â””â”€â”€ part-00000-d40f60ce-5339-488b-b1ea-bf07b16dd907.c000.snappy.parquet\n",
      "â””â”€â”€ _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# === Directory-based Partitioning ===\n",
    "print(\"=== 2. Partitioning by DEST_COUNTRY_NAME ===\")\n",
    "out_dir_partition_parquet = str(Path(get_project_path('lessons', '05_io', 'temp', 'partition_parquet')))\n",
    "\n",
    "# Partition by destination country\n",
    "csv_df.limit(10).write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(out_dir_partition_parquet)\n",
    "\n",
    "print(\"\\nðŸ‘‰ **Directory structure created:**\")\n",
    "\n",
    "# Show the directory structure\n",
    "def show_directory_tree(path, prefix=\"\", max_files=3):\n",
    "    \"\"\"Display directory tree structure\"\"\"\n",
    "    try:\n",
    "        items = sorted(os.listdir(path))\n",
    "        dirs = [item for item in items if os.path.isdir(os.path.join(path, item))]\n",
    "        files = [item for item in items if os.path.isfile(os.path.join(path, item)) and not item.startswith('.')]\n",
    "        \n",
    "        # Show directories (partitions)\n",
    "        for i, dir_name in enumerate(dirs):\n",
    "            is_last_dir = i == len(dirs) - 1 and len(files) == 0\n",
    "            print(f\"{prefix}{'â””â”€â”€ ' if is_last_dir else 'â”œâ”€â”€ '}{dir_name}/\")\n",
    "            \n",
    "            # Show files in each partition directory\n",
    "            subpath = os.path.join(path, dir_name)\n",
    "            if os.path.isdir(subpath):\n",
    "                subfiles = [f for f in os.listdir(subpath) if not f.startswith('.')]\n",
    "                for j, filename in enumerate(subfiles[:max_files]):\n",
    "                    is_last_file = j == len(subfiles[:max_files]) - 1\n",
    "                    extension = \"    \" if is_last_dir else \"â”‚   \"\n",
    "                    print(f\"{prefix}{extension}{'â””â”€â”€ ' if is_last_file else 'â”œâ”€â”€ '}{filename}\")\n",
    "                if len(subfiles) > max_files:\n",
    "                    extension = \"    \" if is_last_dir else \"â”‚   \"\n",
    "                    print(f\"{prefix}{extension}â””â”€â”€ ... ({len(subfiles) - max_files} more files)\")\n",
    "        \n",
    "        # Show root-level files\n",
    "        for i, filename in enumerate(files[:max_files]):\n",
    "            is_last = i == len(files[:max_files]) - 1\n",
    "            print(f\"{prefix}{'â””â”€â”€ ' if is_last else 'â”œâ”€â”€ '}{filename}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{prefix}Error reading directory: {e}\")\n",
    "\n",
    "show_directory_tree(out_dir_partition_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f44d9",
   "metadata": {},
   "source": [
    "### **3. Bucketing (Table-based Organization)**\n",
    "\n",
    "**Key Concepts**:\n",
    "- Groups rows into fixed number of **buckets** by column hash\n",
    "- Helps reduce shuffle in future joins\n",
    "- Requires managed tables (saveAsTable)\n",
    "- Pre-distributes data for optimal join performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4774dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3. Bucketing by 'count' column ===\n",
      "âœ… Bucketed table 'bucketedFiles' created with 10 buckets\n",
      "\n",
      "Bucketed table contents:\n",
      "\n",
      "ðŸ“Š Table info:\n",
      "   Rows: 255\n",
      "   Partitions: 10\n",
      "\n",
      "ðŸ”§ **Table Metadata:**\n",
      "+--------------+---------+-------+\n",
      "|col_name      |data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|Num Buckets   |10       |       |\n",
      "|Bucket Columns|[`count`]|       |\n",
      "|Sort Columns  |[`count`]|       |\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Bucketing Demonstration ===\n",
    "print(\"=== 3. Bucketing by 'count' column ===\")\n",
    "out_dir_bucket_parquet = str(Path(get_project_path('lessons', '05_io', 'temp', 'bucket_parquet')))\n",
    "\n",
    "# Create bucketed table\n",
    "csv_df.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(10, \"count\") \\\n",
    "    .sortBy(\"count\") \\\n",
    "    .option(\"path\", out_dir_bucket_parquet) \\\n",
    "    .saveAsTable(\"bucketedFiles\")\n",
    "\n",
    "print(\"âœ… Bucketed table 'bucketedFiles' created with 10 buckets\")\n",
    "\n",
    "# Read the bucketed table\n",
    "bucketed_df = spark.table(\"bucketedFiles\")\n",
    "print(\"\\nBucketed table contents:\")\n",
    "# bucketed_df.orderBy(\"count\").show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Table info:\")\n",
    "print(f\"   Rows: {bucketed_df.count()}\")\n",
    "print(f\"   Partitions: {bucketed_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Show table metadata\n",
    "print(\"\\nðŸ”§ **Table Metadata:**\")\n",
    "spark.sql(\"DESCRIBE EXTENDED bucketedFiles\").filter(\n",
    "    col(\"col_name\").isin([\"Num Buckets\", \"Bucket Columns\", \"Sort Columns\"])\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e09406",
   "metadata": {},
   "source": [
    "## ðŸ”Ž **Step 5: Why This Matters?**\n",
    "\n",
    "Understanding these advanced I/O concepts is crucial for building efficient, scalable data pipelines in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb8968",
   "metadata": {},
   "source": [
    "## âœ… **In Simple Words**:\n",
    "\n",
    "**Spark's unified API lets you read/write many formats with consistent syntax:**\n",
    "\n",
    "ðŸ”§ **Formats**:\n",
    "- **CSV/JSON**: Flexible for data exchange\n",
    "- **Parquet/ORC**: Efficient for analytics\n",
    "- **JDBC**: Connects to databases\n",
    "\n",
    "âš¡ **Optimizations**:\n",
    "- **Partitioning**: Organize data in directories â†’ faster filtering\n",
    "- **Bucketing**: Pre-distribute data â†’ faster joins\n",
    "- **Parallel Writing**: Control file output â†’ optimal performance\n",
    "\n",
    "ðŸŽ¯ **Result**: Scalable, high-performance data pipelines for real-world analytics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
